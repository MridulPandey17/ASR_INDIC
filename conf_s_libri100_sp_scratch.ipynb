{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0730023",
   "metadata": {},
   "source": [
    "reference : https://github.com/NVIDIA/NeMo/blob/main/examples/asr/conf/conformer/conformer_ctc_bpe.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9e537e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torchmetrics.text.wer import WordErrorRate\n",
    "from torchmetrics.text.cer import CharErrorRate\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "\n",
    "# Support Libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from itertools import groupby\n",
    "\n",
    "# Python Scripts\n",
    "from conf_model import ConformerEncoder, LSTMDecoder#, ConvASRDecoder\n",
    "from conf_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "135807bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU Devices : 1\n",
      "Using devices : [0]\n"
     ]
    }
   ],
   "source": [
    "total_devices = torch.cuda.device_count()\n",
    "print(f\"Available GPU Devices : {total_devices}\")\n",
    "DEVICES = [i for i in range(0,total_devices)]\n",
    "print(f\"Using devices : {DEVICES}\")\n",
    "DEVICE = f\"cuda:{DEVICES[0]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ded70e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BS = 20*len(DEVICES)\n",
    "TEST_BS = 10*len(DEVICES)\n",
    "EPOCHS = 1000\n",
    "NUM_WORKERS = 4*len(DEVICES)\n",
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e31f85c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       0  \\\n",
      "0      /media/rathna/New Volume/datasets/Librispeech/...   \n",
      "1      /media/rathna/New Volume/datasets/Librispeech/...   \n",
      "2      /media/rathna/New Volume/datasets/Librispeech/...   \n",
      "3      /media/rathna/New Volume/datasets/Librispeech/...   \n",
      "4      /media/rathna/New Volume/datasets/Librispeech/...   \n",
      "...                                                  ...   \n",
      "26747  /media/rathna/New Volume/datasets/Librispeech/...   \n",
      "26748  /media/rathna/New Volume/datasets/Librispeech/...   \n",
      "26749  /media/rathna/New Volume/datasets/Librispeech/...   \n",
      "26750  /media/rathna/New Volume/datasets/Librispeech/...   \n",
      "26751  /media/rathna/New Volume/datasets/Librispeech/...   \n",
      "\n",
      "                                                       1       2      3  \n",
      "0      through which i saw shrubs and a grass plat lo...  15.250  16000  \n",
      "1      and struck his stick on the floor again my mot...  14.500  16000  \n",
      "2      and i felt a kind of panic on seeing the pale ...  12.725  16000  \n",
      "3      that form of government will be generally spea...  15.120  16000  \n",
      "4      in other words i was what is called a distingu...  14.690  16000  \n",
      "...                                                  ...     ...    ...  \n",
      "26747  and the other german planes turned back toward...   8.110  16000  \n",
      "26748  more composed than her previous agitation had ...  14.930  16000  \n",
      "26749                              the child turned back   2.090  16000  \n",
      "26750  but when you get in de market an anybody axe y...   8.680  16000  \n",
      "26751  were thronged with people out upon their sunda...  13.845  16000  \n",
      "\n",
      "[26752 rows x 4 columns]\n",
      "                                                      0  \\\n",
      "0     /media/rathna/New Volume/datasets/Librispeech/...   \n",
      "1     /media/rathna/New Volume/datasets/Librispeech/...   \n",
      "2     /media/rathna/New Volume/datasets/Librispeech/...   \n",
      "3     /media/rathna/New Volume/datasets/Librispeech/...   \n",
      "4     /media/rathna/New Volume/datasets/Librispeech/...   \n",
      "...                                                 ...   \n",
      "2418  /media/rathna/New Volume/datasets/Librispeech/...   \n",
      "2419  /media/rathna/New Volume/datasets/Librispeech/...   \n",
      "2420  /media/rathna/New Volume/datasets/Librispeech/...   \n",
      "2421  /media/rathna/New Volume/datasets/Librispeech/...   \n",
      "2422  /media/rathna/New Volume/datasets/Librispeech/...   \n",
      "\n",
      "                                                      1       2      3  \n",
      "0     little by little however the latter became hem...  11.350  16000  \n",
      "1     and then we insist upon it the study of social...  14.310  16000  \n",
      "2     so you will be a good girl i know and not make...   8.010  16000  \n",
      "3     let us hear the suspicions i will look after t...   3.575  16000  \n",
      "4     this passage then bears out the fact that all ...   4.855  16000  \n",
      "...                                                 ...     ...    ...  \n",
      "2418  the wind never lulls but to acquire increased ...  11.200  16000  \n",
      "2419  she was the most agreeable woman ive ever know...   5.780  16000  \n",
      "2420  the world is all there just as it used to be b...   4.930  16000  \n",
      "2421  she had almost forgotten that it was here with...   3.550  16000  \n",
      "2422  the style and plan of the timaeus differ great...   7.830  16000  \n",
      "\n",
      "[2423 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "metadata_train = pd.read_csv(\"/media/rathna/New Volume/datasets/Librispeech/metadata_train_clean_100.tsv\", sep = '\\t', header = None)\n",
    "metadata_dev = pd.read_csv(\"/media/rathna/New Volume/datasets/Librispeech/metadata_test_clean.tsv\", sep = '\\t', header = None)\n",
    "\n",
    "metadata_train = metadata_train[metadata_train[2]<=16].reset_index(drop=True)\n",
    "\n",
    "\n",
    "metadata_train = metadata_train.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(metadata_train)\n",
    "\n",
    "metadata_dev = metadata_dev[metadata_dev[2]<=16].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "metadata_dev = metadata_dev.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(metadata_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff674169",
   "metadata": {},
   "source": [
    "USE sentence_piece_final.ipynb TO BUILD THE TOKENIZER AND BEFORE RUNNING THE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a6f3b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>\\t0', '<s>\\t0', '</s>\\t0', '▁\\t-1.875', 'e\\t-2.51473', 's\\t-2.74813', 't\\t-2.83066', 'a\\t-2.84131', 'i\\t-2.916', 'o\\t-3.0554', 'r\\t-3.05788', 'l\\t-3.20473', 'd\\t-3.30153', 'h\\t-3.45137', 'u\\t-3.65498', 'm\\t-3.66756', 'c\\t-3.69663', 'n\\t-3.69821', '▁the\\t-3.98861', 'y\\t-4.05982', 'p\\t-4.06069', 'b\\t-4.18664', 'g\\t-4.30007', '▁w\\t-4.30797', 'f\\t-4.32524', 'on\\t-4.64682', 'er\\t-4.66921', 'w\\t-4.78911', '▁and\\t-4.85567', 'k\\t-4.87774', 'en\\t-4.88871', '▁of\\t-4.91166', 'an\\t-4.92246', '▁to\\t-4.94232', 'ing\\t-4.98547', 'v\\t-5.73803', 'x\\t-6.52142', 'j\\t-6.56745', 'q\\t-7.45048', 'z\\t-7.45048', '']\n",
      "['▁', 'e', 's', 't', 'a', 'i', 'o', 'r', 'l', 'd', 'h', 'u', 'm', 'c', 'n', '▁the', 'y', 'p', 'b', 'g', '▁w', 'f', 'on', 'er', 'w', '▁and', 'k', 'en', '▁of', 'an', '▁to', 'ing', 'v', 'x', 'j', 'q', 'z']\n",
      "38\n",
      "Character mappings : {' ': 0, 'a': 1, 'an': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'en': 7, 'er': 8, 'f': 9, 'g': 10, 'h': 11, 'i': 12, 'ing': 13, 'j': 14, 'k': 15, 'l': 16, 'm': 17, 'n': 18, 'o': 19, 'on': 20, 'p': 21, 'q': 22, 'r': 23, 's': 24, 't': 25, 'u': 26, 'v': 27, 'w': 28, 'x': 29, 'y': 30, 'z': 31, '▁': 32, '▁and': 33, '▁of': 34, '▁the': 35, '▁to': 36, '▁w': 37}\n"
     ]
    }
   ],
   "source": [
    "my_file = open(\"/home/rathna/Desktop/tokenizer/libri_100/tokenizer.vocab\", \"r\")\n",
    "  \n",
    "# reading the file\n",
    "data = my_file.read()\n",
    "  \n",
    "\n",
    "vocab = data.split(\"\\n\")\n",
    "print(vocab)\n",
    "\n",
    "vocab_f = []\n",
    "for i in range(3, len(vocab)-1):\n",
    "    vocab_f.append(vocab[i].split(\"\\t\")[0])\n",
    "print(vocab_f)\n",
    "\n",
    "vocab_f = vocab_f+[' ']\n",
    "vocab = sorted(vocab_f)\n",
    "num_embeddings = len(vocab)\n",
    "print(num_embeddings)\n",
    "vocab_ids = [int(i) for i in range(num_embeddings)]\n",
    "vocab_dict = dict(zip(vocab,vocab_ids))\n",
    "print(f\"Character mappings : {vocab_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d46a510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "s = spm.SentencePieceProcessor(model_file='/home/rathna/Desktop/tokenizer/libri_100/tokenizer.model')\n",
    "\n",
    "\n",
    "import re\n",
    "def sent_piece_tokenizer(text):\n",
    "    text_list = re.split(\" \", text)\n",
    "    encoded_words = []\n",
    "    for word in text_list:\n",
    "        temp = s.encode(word, out_type=str, enable_sampling=True, alpha=0.1, nbest_size=-1)\n",
    "        encoded_words += temp\n",
    "        encoded_words.append(' ')\n",
    "    encoded_words = encoded_words[:-1]\n",
    "    #print(encoded_words)\n",
    "    mapped_words = []\n",
    "    for i in range(len(encoded_words)):\n",
    "        #print(i, ':', encoded_words[i])\n",
    "        mapped_words.append(vocab_dict[encoded_words[i]])\n",
    "    return mapped_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "101e4111",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dict = {\n",
    "        \"sample_rate\":16000,\n",
    "        \"n_mels\":80, #as per reference\n",
    "    }\n",
    "\n",
    "# time_masks = [torchaudio.transforms.TimeMasking(time_mask_param=15, p=0.05) for _ in range(10)]\n",
    "# train_transform = nn.Sequential(\n",
    "#    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=80, hop_length=160), #80 filter banks, 25ms window size, 10ms hop\n",
    "#    torchaudio.transforms.FrequencyMasking(freq_mask_param=27),\n",
    "#    *time_masks,\n",
    "#  )\n",
    "train_transform = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=80, hop_length=160)\n",
    "validation_transform = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=80, hop_length=160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe609522",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    The Class will act as the container for our dataset. It will take your dataframe, the root path, and also the transform function for transforming the dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_frame, char_dict, transform=None):\n",
    "        self.data_frame = data_frame\n",
    "        #self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        #self.max_transcript_len = N\n",
    "        self.char_dict = char_dict\n",
    "    def __len__(self):\n",
    "        # Return the length of the dataset\n",
    "        return len(self.data_frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Return the observation based on an index. Ex. dataset[0] will return the first element from the dataset, in this case the image and the label.\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        file_path = self.data_frame.iloc[idx, 0]\n",
    "        waveform, sample_rate = torchaudio.load(file_path, normalize = True)\n",
    "        if sample_rate!=16000:\n",
    "            waveform = transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "        mel_spec = self.transform(waveform) # (batch, n_mels, time)\n",
    "        mel_spec = mel_spec.squeeze(0).transpose(0,1)  # (time, n_mels)\n",
    "        mel_spec_len = ((mel_spec.shape[0] - 1) // 2 - 1) // 2\n",
    "        \n",
    "        transcript = self.data_frame.iloc[idx,1]\n",
    "        transcript = str(transcript)\n",
    "        input_transcript_list = sent_piece_tokenizer(transcript)\n",
    "#         for char in transcript:\n",
    "#             input_transcript_list.append(self.char_dict[char])\n",
    "        \n",
    "        label_tensor = torch.LongTensor(input_transcript_list)\n",
    "        label_len = len(input_transcript_list)\n",
    "    \n",
    "        return mel_spec, label_tensor, mel_spec_len, label_len, transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dedafc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = int(len(metadata)*0.99)\n",
    "# dev_size = len(metadata) - train_size\n",
    "\n",
    "# if not os.path.exists(metadata_path):\n",
    "#     metadata_train = metadata[:train_size]\n",
    "#     metadata_dev   = metadata[train_size:]\n",
    "#     metadata_train.to_csv(\"lj_metadata_train.tsv\", sep='\\t', header=False, index=False)\n",
    "#     metadata_dev.to_csv(\"lj_metadata_test.tsv\", sep='\\t', header=False, index=False)\n",
    "# else:\n",
    "#     print(\"Using existing metadata!\")\n",
    "#     metadata_train = pd.read_csv(\"lj_metadata_train.tsv\", sep='\\t', header=None, encoding='utf-8')\n",
    "#     metadata_dev  = pd.read_csv(\"lj_metadata_test.tsv\", sep='\\t', header=None, encoding='utf-8')\n",
    "    \n",
    "trainset = MyDataset(metadata_train, vocab_dict, transform=train_transform)\n",
    "devset = MyDataset(metadata_dev, vocab_dict, transform=validation_transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = TRAIN_BS, shuffle = True, collate_fn = collate_batch, drop_last=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "devloader  = torch.utils.data.DataLoader(devset,  batch_size = TEST_BS, shuffle = True, collate_fn = collate_batch, drop_last=True, num_workers=NUM_WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1840b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_params = {\n",
    "#     \"d_input\": 80,\n",
    "#     \"d_model\": 144,\n",
    "#     \"num_layers\": 16,\n",
    "#     \"conv_kernel_size\": 32,\n",
    "#     \"dropout\": 0.1,\n",
    "#     \"num_heads\": 4\n",
    "# }\n",
    "\n",
    "# decoder_params = {\n",
    "#     \"d_encoder\": 144,\n",
    "#     \"d_decoder\": 320,\n",
    "#     \"num_layers\": 1,\n",
    "#     \"num_classes\":len(vocab)+1\n",
    "# }\n",
    "\n",
    "encoder_params = {\n",
    "    \"d_input\": 80,\n",
    "    \"d_model\": 176,\n",
    "    \"num_layers\": 16,\n",
    "    \"conv_kernel_size\": 31,\n",
    "    \"dropout\": 0.1,\n",
    "    \"num_heads\": 4\n",
    "}\n",
    "\n",
    "decoder_params = {\n",
    "    \"d_encoder\": 176,\n",
    "    \"d_decoder\": 320,\n",
    "    \"num_layers\": 1,\n",
    "    \"num_classes\":len(vocab)+1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64452f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvASRDecoder(nn.Module):\n",
    "#   '''\n",
    "#     LSTM Decoder\n",
    "#     Parameters:\n",
    "#       d_encoder (int): Output dimension of the encoder\n",
    "#       d_decoder (int): Hidden dimension of the decoder\n",
    "#       num_layers (int): Number of LSTM layers to use in the decoder\n",
    "#       num_classes (int): Number of output classes to predict\n",
    "    \n",
    "#     Inputs:\n",
    "#       x (Tensor): (batch_size, time, d_encoder)\n",
    "    \n",
    "#     Outputs:\n",
    "#       Tensor (batch_size, time, num_classes): Class prediction logits\n",
    "  \n",
    "#   '''\n",
    "    def __init__(self, d_encoder=144, d_decoder=320, num_layers=1, num_classes=29):\n",
    "        super(ConvASRDecoder, self).__init__()\n",
    "        #self.conv = nn.Conv1d(input_size=d_encoder, hidden_size=d_decoder, num_layers=num_layers, batch_first=True)\n",
    "        #in_channels, out_channels, kernel_size,\n",
    "        self.conv = nn.Conv1d(in_channels=d_encoder, out_channels=d_decoder, kernel_size=num_layers, bias=True)\n",
    "        self.linear = nn.Linear(d_decoder, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x.transpose(1, 2)).transpose(1,2)\n",
    "        decoder_cem = x # TRIAL FOR CEM\n",
    "        logits = self.linear(x)\n",
    "        return logits, decoder_cem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73500ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ConformerEncoder(\n",
    "                      d_input=encoder_params['d_input'],\n",
    "                      d_model=encoder_params['d_model'],\n",
    "                      num_layers=encoder_params['num_layers'],\n",
    "                      conv_kernel_size=encoder_params['conv_kernel_size'], \n",
    "                      dropout=encoder_params['dropout'],\n",
    "                      num_heads=encoder_params['num_heads']\n",
    "                    )\n",
    "  \n",
    "# decoder = LSTMDecoder(\n",
    "#                   d_encoder=decoder_params['d_encoder'], \n",
    "#                   d_decoder=decoder_params['d_decoder'], \n",
    "#                   num_layers=decoder_params['num_layers'],\n",
    "#                   num_classes= decoder_params['num_classes'])\n",
    "\n",
    "decoder = ConvASRDecoder(\n",
    "                  d_encoder=decoder_params['d_encoder'], \n",
    "                  d_decoder=decoder_params['d_decoder'], \n",
    "                  num_layers=decoder_params['num_layers'],\n",
    "                  num_classes= decoder_params['num_classes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64a697ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.to(DEVICE)\n",
    "decoder = decoder.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e6b6f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_decoder =  GreedyCharacterDecoder().eval()\n",
    "criterion = nn.CTCLoss(blank=len(vocab), zero_infinity=True)\n",
    "#optimizer = torch.optim.AdamW(list(encoder.parameters()) + list(decoder.parameters()), lr=5e-4, betas=(.9, .98), eps = 1e-09, weight_decay=1e-6)\n",
    "\n",
    "#CHANGED FROM NEMO\n",
    "\n",
    "optimizer = torch.optim.AdamW(list(encoder.parameters()) + list(decoder.parameters()), lr=5.0, betas=(.9, .98), eps = 1e-09, weight_decay=1e-3)\n",
    "scheduler = TransformerLrScheduler(optimizer, encoder_params['d_model'], 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23f5f082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder - num_params: 14.71M,  size: 56.1MB\n",
      "Decoder - num_params: 0.07M,  size: 0.26MB\n"
     ]
    }
   ],
   "source": [
    "model_size(encoder, 'Encoder')\n",
    "model_size(decoder, 'Decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4068933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConformerEncoder(\n",
      "  (conv_subsample): Conv2dSubsampling(\n",
      "    (conv2d_1): Conv2d(1, 176, kernel_size=(3, 3), stride=(2, 2))\n",
      "    (relu): ReLU()\n",
      "    (conv2d_2): Conv2d(176, 176, kernel_size=(3, 3), stride=(2, 2))\n",
      "  )\n",
      "  (linear_proj): Linear(in_features=3344, out_features=176, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0): ConformerBlock(\n",
      "      (ff1): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attention): RelativeMultiHeadAttention(\n",
      "        (W_q): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_k): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_v): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_pos): Linear(in_features=176, out_features=176, bias=False)\n",
      "        (W_out): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (positional_encoder): PositionalEncoder()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (conv_block): ConvBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (conv1d_1): Conv1d(176, 352, kernel_size=(1,), stride=(1,))\n",
      "        (glu): GLU(dim=1)\n",
      "        (conv1d_2): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)\n",
      "        (batch_norm1d): BatchNorm1d(176, eps=6.1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (silu): SiLU()\n",
      "        (conv1d_3): Conv1d(176, 176, kernel_size=(1,), stride=(1,))\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff2): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): ConformerBlock(\n",
      "      (ff1): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attention): RelativeMultiHeadAttention(\n",
      "        (W_q): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_k): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_v): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_pos): Linear(in_features=176, out_features=176, bias=False)\n",
      "        (W_out): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (positional_encoder): PositionalEncoder()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (conv_block): ConvBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (conv1d_1): Conv1d(176, 352, kernel_size=(1,), stride=(1,))\n",
      "        (glu): GLU(dim=1)\n",
      "        (conv1d_2): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)\n",
      "        (batch_norm1d): BatchNorm1d(176, eps=6.1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (silu): SiLU()\n",
      "        (conv1d_3): Conv1d(176, 176, kernel_size=(1,), stride=(1,))\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff2): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): ConformerBlock(\n",
      "      (ff1): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attention): RelativeMultiHeadAttention(\n",
      "        (W_q): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_k): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_v): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_pos): Linear(in_features=176, out_features=176, bias=False)\n",
      "        (W_out): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (positional_encoder): PositionalEncoder()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (conv_block): ConvBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (conv1d_1): Conv1d(176, 352, kernel_size=(1,), stride=(1,))\n",
      "        (glu): GLU(dim=1)\n",
      "        (conv1d_2): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)\n",
      "        (batch_norm1d): BatchNorm1d(176, eps=6.1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (silu): SiLU()\n",
      "        (conv1d_3): Conv1d(176, 176, kernel_size=(1,), stride=(1,))\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff2): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): ConformerBlock(\n",
      "      (ff1): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attention): RelativeMultiHeadAttention(\n",
      "        (W_q): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_k): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_v): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_pos): Linear(in_features=176, out_features=176, bias=False)\n",
      "        (W_out): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (positional_encoder): PositionalEncoder()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (conv_block): ConvBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (conv1d_1): Conv1d(176, 352, kernel_size=(1,), stride=(1,))\n",
      "        (glu): GLU(dim=1)\n",
      "        (conv1d_2): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)\n",
      "        (batch_norm1d): BatchNorm1d(176, eps=6.1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (silu): SiLU()\n",
      "        (conv1d_3): Conv1d(176, 176, kernel_size=(1,), stride=(1,))\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff2): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): ConformerBlock(\n",
      "      (ff1): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attention): RelativeMultiHeadAttention(\n",
      "        (W_q): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_k): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_v): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_pos): Linear(in_features=176, out_features=176, bias=False)\n",
      "        (W_out): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (positional_encoder): PositionalEncoder()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (conv_block): ConvBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (conv1d_1): Conv1d(176, 352, kernel_size=(1,), stride=(1,))\n",
      "        (glu): GLU(dim=1)\n",
      "        (conv1d_2): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)\n",
      "        (batch_norm1d): BatchNorm1d(176, eps=6.1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (silu): SiLU()\n",
      "        (conv1d_3): Conv1d(176, 176, kernel_size=(1,), stride=(1,))\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff2): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): ConformerBlock(\n",
      "      (ff1): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attention): RelativeMultiHeadAttention(\n",
      "        (W_q): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_k): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_v): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_pos): Linear(in_features=176, out_features=176, bias=False)\n",
      "        (W_out): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (positional_encoder): PositionalEncoder()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (conv_block): ConvBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (conv1d_1): Conv1d(176, 352, kernel_size=(1,), stride=(1,))\n",
      "        (glu): GLU(dim=1)\n",
      "        (conv1d_2): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)\n",
      "        (batch_norm1d): BatchNorm1d(176, eps=6.1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (silu): SiLU()\n",
      "        (conv1d_3): Conv1d(176, 176, kernel_size=(1,), stride=(1,))\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff2): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (6): ConformerBlock(\n",
      "      (ff1): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attention): RelativeMultiHeadAttention(\n",
      "        (W_q): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_k): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_v): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_pos): Linear(in_features=176, out_features=176, bias=False)\n",
      "        (W_out): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (positional_encoder): PositionalEncoder()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (conv_block): ConvBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (conv1d_1): Conv1d(176, 352, kernel_size=(1,), stride=(1,))\n",
      "        (glu): GLU(dim=1)\n",
      "        (conv1d_2): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)\n",
      "        (batch_norm1d): BatchNorm1d(176, eps=6.1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (silu): SiLU()\n",
      "        (conv1d_3): Conv1d(176, 176, kernel_size=(1,), stride=(1,))\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff2): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (7): ConformerBlock(\n",
      "      (ff1): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attention): RelativeMultiHeadAttention(\n",
      "        (W_q): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_k): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_v): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_pos): Linear(in_features=176, out_features=176, bias=False)\n",
      "        (W_out): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (positional_encoder): PositionalEncoder()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (conv_block): ConvBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (conv1d_1): Conv1d(176, 352, kernel_size=(1,), stride=(1,))\n",
      "        (glu): GLU(dim=1)\n",
      "        (conv1d_2): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)\n",
      "        (batch_norm1d): BatchNorm1d(176, eps=6.1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (silu): SiLU()\n",
      "        (conv1d_3): Conv1d(176, 176, kernel_size=(1,), stride=(1,))\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff2): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (8): ConformerBlock(\n",
      "      (ff1): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attention): RelativeMultiHeadAttention(\n",
      "        (W_q): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_k): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_v): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_pos): Linear(in_features=176, out_features=176, bias=False)\n",
      "        (W_out): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (positional_encoder): PositionalEncoder()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (conv_block): ConvBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (conv1d_1): Conv1d(176, 352, kernel_size=(1,), stride=(1,))\n",
      "        (glu): GLU(dim=1)\n",
      "        (conv1d_2): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)\n",
      "        (batch_norm1d): BatchNorm1d(176, eps=6.1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (silu): SiLU()\n",
      "        (conv1d_3): Conv1d(176, 176, kernel_size=(1,), stride=(1,))\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff2): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (9): ConformerBlock(\n",
      "      (ff1): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attention): RelativeMultiHeadAttention(\n",
      "        (W_q): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_k): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_v): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_pos): Linear(in_features=176, out_features=176, bias=False)\n",
      "        (W_out): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (positional_encoder): PositionalEncoder()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (conv_block): ConvBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (conv1d_1): Conv1d(176, 352, kernel_size=(1,), stride=(1,))\n",
      "        (glu): GLU(dim=1)\n",
      "        (conv1d_2): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)\n",
      "        (batch_norm1d): BatchNorm1d(176, eps=6.1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (silu): SiLU()\n",
      "        (conv1d_3): Conv1d(176, 176, kernel_size=(1,), stride=(1,))\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff2): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (10): ConformerBlock(\n",
      "      (ff1): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attention): RelativeMultiHeadAttention(\n",
      "        (W_q): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_k): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_v): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_pos): Linear(in_features=176, out_features=176, bias=False)\n",
      "        (W_out): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (positional_encoder): PositionalEncoder()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (conv_block): ConvBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (conv1d_1): Conv1d(176, 352, kernel_size=(1,), stride=(1,))\n",
      "        (glu): GLU(dim=1)\n",
      "        (conv1d_2): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)\n",
      "        (batch_norm1d): BatchNorm1d(176, eps=6.1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (silu): SiLU()\n",
      "        (conv1d_3): Conv1d(176, 176, kernel_size=(1,), stride=(1,))\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff2): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (11): ConformerBlock(\n",
      "      (ff1): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attention): RelativeMultiHeadAttention(\n",
      "        (W_q): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_k): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_v): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_pos): Linear(in_features=176, out_features=176, bias=False)\n",
      "        (W_out): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (positional_encoder): PositionalEncoder()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (conv_block): ConvBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (conv1d_1): Conv1d(176, 352, kernel_size=(1,), stride=(1,))\n",
      "        (glu): GLU(dim=1)\n",
      "        (conv1d_2): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)\n",
      "        (batch_norm1d): BatchNorm1d(176, eps=6.1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (silu): SiLU()\n",
      "        (conv1d_3): Conv1d(176, 176, kernel_size=(1,), stride=(1,))\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff2): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (12): ConformerBlock(\n",
      "      (ff1): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attention): RelativeMultiHeadAttention(\n",
      "        (W_q): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_k): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_v): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_pos): Linear(in_features=176, out_features=176, bias=False)\n",
      "        (W_out): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (positional_encoder): PositionalEncoder()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (conv_block): ConvBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (conv1d_1): Conv1d(176, 352, kernel_size=(1,), stride=(1,))\n",
      "        (glu): GLU(dim=1)\n",
      "        (conv1d_2): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)\n",
      "        (batch_norm1d): BatchNorm1d(176, eps=6.1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (silu): SiLU()\n",
      "        (conv1d_3): Conv1d(176, 176, kernel_size=(1,), stride=(1,))\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff2): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (13): ConformerBlock(\n",
      "      (ff1): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attention): RelativeMultiHeadAttention(\n",
      "        (W_q): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_k): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_v): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_pos): Linear(in_features=176, out_features=176, bias=False)\n",
      "        (W_out): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (positional_encoder): PositionalEncoder()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (conv_block): ConvBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (conv1d_1): Conv1d(176, 352, kernel_size=(1,), stride=(1,))\n",
      "        (glu): GLU(dim=1)\n",
      "        (conv1d_2): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)\n",
      "        (batch_norm1d): BatchNorm1d(176, eps=6.1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (silu): SiLU()\n",
      "        (conv1d_3): Conv1d(176, 176, kernel_size=(1,), stride=(1,))\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff2): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (14): ConformerBlock(\n",
      "      (ff1): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attention): RelativeMultiHeadAttention(\n",
      "        (W_q): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_k): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_v): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_pos): Linear(in_features=176, out_features=176, bias=False)\n",
      "        (W_out): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (positional_encoder): PositionalEncoder()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (conv_block): ConvBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (conv1d_1): Conv1d(176, 352, kernel_size=(1,), stride=(1,))\n",
      "        (glu): GLU(dim=1)\n",
      "        (conv1d_2): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)\n",
      "        (batch_norm1d): BatchNorm1d(176, eps=6.1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (silu): SiLU()\n",
      "        (conv1d_3): Conv1d(176, 176, kernel_size=(1,), stride=(1,))\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff2): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (15): ConformerBlock(\n",
      "      (ff1): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (attention): RelativeMultiHeadAttention(\n",
      "        (W_q): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_k): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_v): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (W_pos): Linear(in_features=176, out_features=176, bias=False)\n",
      "        (W_out): Linear(in_features=176, out_features=176, bias=True)\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (positional_encoder): PositionalEncoder()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (conv_block): ConvBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (conv1d_1): Conv1d(176, 352, kernel_size=(1,), stride=(1,))\n",
      "        (glu): GLU(dim=1)\n",
      "        (conv1d_2): Conv1d(176, 176, kernel_size=(31,), stride=(1,), padding=same, groups=176)\n",
      "        (batch_norm1d): BatchNorm1d(176, eps=6.1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (silu): SiLU()\n",
      "        (conv1d_3): Conv1d(176, 176, kernel_size=(1,), stride=(1,))\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ff2): FeedForwardBlock(\n",
      "        (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "        (linear_1): Linear(in_features=176, out_features=704, bias=True)\n",
      "        (silu): SiLU()\n",
      "        (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "        (linear_2): Linear(in_features=704, out_features=176, bias=True)\n",
      "        (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((176,), eps=6.1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "ConvASRDecoder(\n",
      "  (conv): Conv1d(176, 320, kernel_size=(1,), stride=(1,))\n",
      "  (linear): Linear(in_features=320, out_features=39, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(encoder)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64f53f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d832c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(DEVICE)\n",
    "criterion = criterion.to(DEVICE)\n",
    "char_decoder = char_decoder.to(DEVICE)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "checkpoint_load_path = \"conf_s_libri_best_wer.pt\"\n",
    "\n",
    "if(os.path.exists(checkpoint_load_path)):\n",
    "    start_epoch, best_loss, best_wer = load_checkpoint(encoder, decoder, optimizer, scheduler, checkpoint_load_path, DEVICE)\n",
    "    print(f'Resuming training from checkpoint starting at epoch {start_epoch}.')\n",
    "    print(f'Current model WER : {best_wer}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dac388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, char_decoder, optimizer, scheduler, criterion, grad_scaler, train_loader, device): \n",
    "    wer = WordErrorRate()\n",
    "    cer = CharErrorRate()\n",
    "    # batch accumulation parameter #https://kozodoi.me/blog/20210219/gradient-accumulation\n",
    "    accum_iter = 4  #ADDED\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    avg_loss = 0\n",
    "    avg_wer = 0\n",
    "    avg_cer = 0\n",
    "    batch_count = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch_count += 1\n",
    "        scheduler.step()\n",
    "        gc.collect()\n",
    "        spectrograms, labels, input_lengths, label_lengths, references, mask = batch \n",
    "\n",
    "        spectrograms = spectrograms.squeeze(1).to(device)\n",
    "        labels = labels.to(device)\n",
    "        input_lengths = torch.tensor(input_lengths).to(device)\n",
    "        label_lengths = torch.tensor(label_lengths).to(device)\n",
    "        mask = mask.to(device)\n",
    "    \n",
    "        outputs, _ = encoder(spectrograms, mask)\n",
    "        outputs, _ = decoder(outputs)\n",
    "        loss = criterion(F.log_softmax(outputs, dim=-1).transpose(0, 1), labels, input_lengths, label_lengths)\n",
    "        #loss.backward()\n",
    "        # normalize loss to account for batch accumulation\n",
    "        loss = loss / accum_iter #ADDED\n",
    "        \n",
    "        \n",
    "        grad_scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(encoder.parameters(), 0.1)\n",
    "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), 0.1)\n",
    "        #if (i+1) % args.accumulate_iters == 0:\n",
    "        if ((batch_count) % accum_iter == 0) or (batch_count == len(train_loader)):#ADDED\n",
    "            grad_scaler.step(optimizer)\n",
    "            grad_scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        #avg_loss.update(loss.detach().item())\n",
    "        \n",
    "        #use for small datasets\n",
    "        \n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "        avg_loss += loss.detach().item()\n",
    "        \n",
    "        inds = char_decoder(outputs.detach())\n",
    "        # print(\"train shape:\",inds.shape)\n",
    "        predictions = []\n",
    "        for sample in inds:\n",
    "            #print(sample.shape)\n",
    "            predictions.append(int_to_text(sample, len(vocab), vocab))\n",
    "        avg_wer += wer(predictions, references) * 100\n",
    "        avg_cer += cer(predictions, references) * 100\n",
    "\n",
    "    avg_loss = avg_loss/batch_count\n",
    "    avg_wer = avg_wer/batch_count\n",
    "    avg_cer = avg_cer/batch_count\n",
    "    print(f'Avg WER: {avg_wer}%, Avg Loss: {avg_loss}')  \n",
    "    for i in range(5):\n",
    "        print('Prediction: ', predictions[i])\n",
    "        print('Reference: ', references[i])\n",
    "    \n",
    "    # Print metrics and predictions \n",
    "    del spectrograms, labels, input_lengths, label_lengths, references, outputs, inds, predictions\n",
    "    return avg_wer, avg_cer, avg_loss\n",
    "    \n",
    "def validate(encoder, decoder, char_decoder, criterion, test_loader, device):\n",
    "    ''' Evaluate model on test dataset. '''\n",
    "\n",
    "    wer = WordErrorRate()\n",
    "    cer = CharErrorRate()\n",
    "        \n",
    "    avg_loss = 0\n",
    "    avg_wer = 0\n",
    "    batch_count = 0\n",
    "    avg_cer = 0\n",
    "    \n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    for batch in tqdm(test_loader):\n",
    "        gc.collect()\n",
    "        batch_count += 1\n",
    "        spectrograms, labels, input_lengths, label_lengths, references, mask = batch \n",
    "  \n",
    "    # Move to GPU\n",
    "        spectrograms = spectrograms.to(device)\n",
    "        labels = labels.to(device)\n",
    "        input_lengths = torch.tensor(input_lengths).to(device)\n",
    "        label_lengths = torch.tensor(label_lengths).to(device)\n",
    "        mask = mask.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs, _ = encoder(spectrograms, mask)\n",
    "            outputs, _ = decoder(outputs)\n",
    "            loss = criterion(F.log_softmax(outputs, dim=-1).transpose(0, 1), labels, input_lengths, label_lengths)\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            inds = char_decoder(outputs.detach())\n",
    "            # print(\"validation shape:\", inds.shape)\n",
    "            predictions = []\n",
    "            for sample in inds:\n",
    "                predictions.append(int_to_text(sample, len(vocab), vocab))\n",
    "\n",
    "            avg_wer += wer(predictions, references) * 100\n",
    "            avg_cer += cer(predictions, references) * 100\n",
    "    print(\".............................TEST PREDICTIONS...........................\")\n",
    "    for i in range(5):\n",
    "        print('Prediction: ', predictions[i])\n",
    "        print('Reference: ', references[i])\n",
    "    print(\"************************************************************************\")\n",
    "    \n",
    "    return avg_wer/batch_count, avg_cer/batch_count, loss/batch_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3be84cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_save_path = \"conf_s_libri_chk.pt\"\n",
    "best_loss_save_path = \"conf_s_libri_best_loss.pt\"\n",
    "best_wer_save_path = \"conf_s_libri_best_wer.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "297b12ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Mixed Precision\n",
      "Epoch : 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1337/1337 [12:53<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg WER: 99.84564208984375%, Avg Loss: 0.7495051676779428\n",
      "Prediction:   t   h  t          t  h   t ss t s     h t h h  \n",
      "Reference:  before the majesty of the law and the damning evidence of his guilt despite his social standing and the wealth to sustain it he sees himself alone without friend or sympathiser\n",
      "Prediction:        ta h t t  ta t  e a h th  ta   t t       t e h  t  t     \n",
      "Reference:  while he scarce knew if he were the more impressed with her launching it under missus stringhams nose or with her hope that he would allow to london the honour of discovery the less expansive of the white waistcoats propounded the theory that they saw in london\n",
      "Prediction:     s s                      a s   t  \n",
      "Reference:  perceiving her still to look doubtful and grave he added though frederick does not leave bath with us he will probably remain but a very short time perhaps only a few days behind us\n",
      "Prediction:    h  t        s  t  a t  h  h t  \n",
      "Reference:  yet obtaining the information he desired let us proceed he said to polyte who had not moved since his wife had been taken from the room\n",
      "Prediction:         h h  h h  h  h  h  t    t      h  h     t  \n",
      "Reference:  go and thank him it is well worth it elsie sought him out where he stood alone in a corner an amused spectator of the merry scene see papa she said holding up her hand i think it very beautiful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 242/242 [00:36<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................TEST PREDICTIONS...........................\n",
      "Prediction:   he ho a a a  o \n",
      "Reference:  those fellows are all very loyal even mainhall\n",
      "Prediction:   h h ta a ae to h to a t ta he o \n",
      "Reference:  these escapades are not for old gamewell lad his day has come to twilight\n",
      "Prediction:    h    o h to ha a h   h       \n",
      "Reference:  to meet the needs of this conflict wretchedness has invented a language of combat which is slang\n",
      "Prediction:      h    ta      a  h h to a ho  ta  t\n",
      "Reference:  and hence we find the same sort of clumsiness in the timaeus of plato which characterizes the philosophical poem of lucretius\n",
      "Prediction:   h  to  ta to h  e ho h to ta  \n",
      "Reference:  at once the goat gave a leap escaped from the soldiers and with bowed head rushed upon the boolooroo\n",
      "************************************************************************\n",
      "Epoch 0 - Valid WER: 98.57537841796875%, Valid CER: 77.99987030029297%, Valid Loss: 0.011167665012180805, Train WER: 99.84564208984375%, Train CER: 94.94072723388672%, Train Loss: 0.7495051676779428\n",
      "Validation loss improved, saving best model.\n",
      "Saving checkpoint at epoch:1\n",
      "Validation WER improved, saving best model.\n",
      "Epoch : 2/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1337/1337 [11:22<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg WER: 98.8380126953125%, Avg Loss: 0.6544259240935788\n",
      "Prediction:   oo o o h o  o o o  o o o o o o\n",
      "Reference:  then instead of applying as he should have done to the states general who sate close to his own door\n",
      "Prediction:   o o  o o o o o  o o o o o o o o o o o e o o o o  os o  o  o o o o  o\n",
      "Reference:  our selling the three hundred cakes corrected rebecca you did as much as i no i didnt rebecca randall i just sat at the gate and held the horse yes but whose horse was it that took us to north riverboro\n",
      "Prediction:    o o o e se e o o  o o o o o o o o e o o o o o o o a oe\n",
      "Reference:  with this luigi purchased books and pencils he applied his imitative powers to everything and like giotto when young he drew on his slate sheep houses and trees\n",
      "Prediction:   o o o o o o o o o o   oe   ao o o o o  o o o o o h o o o o o o he o e o o\n",
      "Reference:  i know theres them as is born t own the land and them as is born to sweat ont here missus poyser paused to gasp a little and i know its christened folkss duty to submit to their betters as fur as flesh and\n",
      "Prediction:   o o o  o o o o o o o o o  o o o o o o o o o  o o  o o o   o o oe\n",
      "Reference:  under other circumstances than those which invested him it is not impossible that he would have become a painter the field of sculpture although in its nature rigidly poetical\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 242/242 [00:37<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................TEST PREDICTIONS...........................\n",
      "Prediction:   oo o o o o  oe oe\n",
      "Reference:  alexander groaned i meant to but somehow i couldnt\n",
      "Prediction:    o o o e o o o o o e o o o o o  \n",
      "Reference:  and with it i leave you a name sif the friendly i shall hope to drink with you sometime in valhalla\n",
      "Prediction:   o o  o oo o o o o  o o o o o o oo oe\n",
      "Reference:  he wouldnt search so dont worry replied cyril quietly and the two looked at each other and knew that it was so\n",
      "Prediction:   oe e o o e o e o e th o a e o o o o o o oe \n",
      "Reference:  to do so is to lose god altogether because god becomes intolerable when we seek to measure and to comprehend his infinite majesty\n",
      "Prediction:   o o o o o o o  oe o o o o o o o o o o o o o o o o oe oe oe o o o\n",
      "Reference:  the large letter contains indeed entirely feeble and ill drawn figures that is merely childish and failing work of an inferior hand it is not characteristic of gothic or any other school\n",
      "************************************************************************\n",
      "Epoch 1 - Valid WER: 101.53900909423828%, Valid CER: 76.4940185546875%, Valid Loss: 0.011229224503040314, Train WER: 98.8380126953125%, Train CER: 76.70665740966797%, Train Loss: 0.6544259240935788\n",
      "Epoch : 3/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1337/1337 [11:00<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg WER: 100.68856048583984%, Avg Loss: 0.6483278477824974\n",
      "Prediction:  a a a a a a a a a a a o a a a a a a a a a a a a a a a a a a a a a a a a a a o\n",
      "Reference:  to set off the dignity of the prince and for the greater glory of the kings majesty then might not your worship said she be one of those that without stirring a step serve their king and lord in his court\n",
      "Prediction:  h a a a a a a a es a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a se\n",
      "Reference:  it was his habit to go from one fore castle mess to another and to insist upon having rather more than his share of the choice morsels from each in a short time he came to the repair shop very much the worse for wear with an impaired digestion\n",
      "Prediction:  h a a a a a a a a a a a a a a a a a a a e a a a a o a a a sa sa a a a a o a a a a a a a a a a a a a\n",
      "Reference:  closely following those of the early french supple airships there are several other craft which have become more or less recognised by the german nation as substantial units of war such as the ruthemberg siemens schukert and so forth\n",
      "Prediction:  a a a a a a sa a a a a a a a a a o a\n",
      "Reference:  and her big blackish blue eyes sparkled but she hadnt been laughing before or sparkling either\n",
      "Prediction:   a a a a a a a h a a a a i sa   aa a a a a a h a a a a i a a a a a a sa sh sa a a\n",
      "Reference:  and you may be very certain that i shall avail myself of it as soon as possible they were all astonished and mister bennet who could by no means wish for so speedy a return\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 242/242 [00:36<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................TEST PREDICTIONS...........................\n",
      "Prediction:   a a a a a si\n",
      "Reference:  take him out thorkel and let him taste your sword\n",
      "Prediction:  h a a a a a a a ai a  a\n",
      "Reference:  i made her for only twenty oars because i thought few men would follow me for i was young fifteen years old\n",
      "Prediction:   a a a a a h a a \n",
      "Reference:  to all these inquiries the count responded in the affirmative\n",
      "Prediction:  h a a a a a a ad a sha e a a a a a a a a a a a a a a i\n",
      "Reference:  a stage meal is popular because it proves to the audience that the actors even when called charles hawtrey or owen nares are real people just like you and me\n",
      "Prediction:   ai h     o o a i\n",
      "Reference:  it was in a corner that he lay among weeds and nettles\n",
      "************************************************************************\n",
      "Epoch 2 - Valid WER: 100.50077819824219%, Valid CER: 76.22796630859375%, Valid Loss: 0.01138945110142231, Train WER: 100.68856048583984%, Train CER: 75.2426986694336%, Train Loss: 0.6483278477824974\n",
      "Epoch : 4/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1337/1337 [10:44<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg WER: 102.62889099121094%, Avg Loss: 0.6509268800982273\n",
      "Prediction:       o o   o    o o     h     o o o  \n",
      "Reference:  and rough and tumble nature of the social boom the boom as in itself required that would be the note the subject of the process a comparatively minor question anything was boomable enough when nothing else was more so\n",
      "Prediction:                \n",
      "Reference:  it was he who invented the athletes admirable rules\n",
      "Prediction:     o  o o o o     o o o o   o  o  o  o o o       o o \n",
      "Reference:  the crayfish stuck his tail into the mud he often did this when he was surprised it seemed to help him think when he had thought for a while he waved his big pinching claws and said\n",
      "Prediction:     o o  o       o            o            o o  \n",
      "Reference:  not from any intention on the part of the discoverer since neither he who buried the gold nor he who worked in the field intended that the money should be found but as i said it happened\n",
      "Prediction:    o o o o o o o o o o o o o o o o o o o   o o o o  o o o o  \n",
      "Reference:  if the greatness of christ is his being fatherless then adam is greater than christ for he had neither father nor mother it is said in the old testament and the lord god formed man of the dust of the ground\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 242/242 [00:37<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................TEST PREDICTIONS...........................\n",
      "Prediction:      \n",
      "Reference:  men should not speculate about the nature of god\n",
      "Prediction:    o o o  o o      \n",
      "Reference:  a montfichet a montfichet gamewell to the rescue\n",
      "Prediction:          \n",
      "Reference:  could it mean to last a love set pendulous between sorrow and sorrow\n",
      "Prediction:      o   o o   o  o o o o   o o      \n",
      "Reference:  philip therefore read diligently in the astor library planned literary works that should compel attention and nursed his genius\n",
      "Prediction:       o     o        \n",
      "Reference:  i refer to the thermometer it indicates the figure is obliterated\n",
      "************************************************************************\n",
      "Epoch 3 - Valid WER: 99.98868560791016%, Valid CER: 81.5785903930664%, Valid Loss: 0.010802826844155788, Train WER: 102.62889099121094%, Train CER: 75.58157348632812%, Train Loss: 0.6509268800982273\n",
      "Validation loss improved, saving best model.\n",
      "Saving checkpoint at epoch:4\n",
      "Epoch : 5/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1337/1337 [11:45<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg WER: 102.01261138916016%, Avg Loss: 0.6558482706680526\n",
      "Prediction:   a a aa o a a a aa a aa a aa a a aa a a a a a a a a a a a a a a a o\n",
      "Reference:  judge mullowny addressed the prisoners with many high sounding words about the seriousness of obstructing the traffic in the national capital and inadvertently slipped into a discourse on russia and the dangers of revolution\n",
      "Prediction:   a o a a a a a a a a aa a ao a a a a aa  aa aa o a a a aa a a a a a o o a \n",
      "Reference:  who can follow an animal which can traverse the sea of ice and inhabit caves and dens where no man would venture to intrude\n",
      "Prediction:   a a aa a a a a a a a a aa a a a a a a a a aa a a oa a a a a a a a a a a a\n",
      "Reference:  had struck deadly blows at the heart of each others empire and harried the inmost provinces up to the gates of each others capitals the persian had turned the wild hordes of the avars loose on thrace\n",
      "Prediction:  h a a a a a a a a  a a a a a a a oa aa a aa a a a a a a a a aa a a a a a a\n",
      "Reference:  in manys looks the false hearts history is writ in moods and frowns and wrinkles strange but heaven in thy creation did decree that in thy face sweet love should ever dwell\n",
      "Prediction:  h  a a a aa o a a o a a a a a  a a a a a aa a a a aa a a a o a a a  o\n",
      "Reference:  my attempts to walk resulted in a variety of hops which took me clear of the ground a couple of feet at each step and landed me sprawling upon my face or back\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 242/242 [00:38<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................TEST PREDICTIONS...........................\n",
      "Prediction:   a   \n",
      "Reference:  robin entered the hut dragging the unwilling esquire after him\n",
      "Prediction:   a  a \n",
      "Reference:  indeed there were only one or two strangers who could be admitted among the sisters without producing the same result\n",
      "Prediction:   a a \n",
      "Reference:  nancys curly chestnut crop shone in the sun and olives thick black plaits looked blacker by contrast\n",
      "Prediction:   a  a \n",
      "Reference:  tis now winter out of doors thought the tree\n",
      "Prediction:   a  a \n",
      "Reference:  by reason and affection\n",
      "************************************************************************\n",
      "Epoch 4 - Valid WER: 98.49299621582031%, Valid CER: 92.96858215332031%, Valid Loss: 0.012517726980149746, Train WER: 102.01261138916016%, Train CER: 77.20291137695312%, Train Loss: 0.6558482706680526\n",
      "Validation WER improved, saving best model.\n",
      "Epoch : 6/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███                                     | 104/1337 [00:57<11:21,  1.81it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 25\u001b[0m\n\u001b[1;32m     16\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#variational noise for regularization - COMMENTING ON MAY 19\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     add_model_noise(encoder, std=variational_noise_std, gpu=True)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#     add_model_noise(decoder, std=variational_noise_std, gpu=True)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Train/Validation loops\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m#wer, cer, loss = train(encoder_parallel, decoder_parallel, char_decoder, optimizer, scheduler, criterion, trainloader, DEVICE) \u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     wer, cer, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_decoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_scaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     26\u001b[0m     valid_wer, valid_cer, valid_loss \u001b[38;5;241m=\u001b[39m validate(encoder, decoder, char_decoder, criterion, devloader, DEVICE)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Valid WER: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_wer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%, Valid CER: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_cer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%, Valid Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train WER: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%, Train CER: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)  \n",
      "Cell \u001b[0;32mIn [19], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, char_decoder, optimizer, scheduler, criterion, grad_scaler, train_loader, device)\u001b[0m\n\u001b[1;32m     22\u001b[0m label_lengths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(label_lengths)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     23\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 25\u001b[0m outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspectrograms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m outputs, _ \u001b[38;5;241m=\u001b[39m decoder(outputs)\n\u001b[1;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(F\u001b[38;5;241m.\u001b[39mlog_softmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m), labels, input_lengths, label_lengths)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/rathna/New Volume/rathna_code/word_piece/conf_model.py:377\u001b[0m, in \u001b[0;36mConformerEncoder.forward\u001b[0;34m(self, x, mask, p)\u001b[0m\n\u001b[1;32m    374\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39mp, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# added for DUST\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 377\u001b[0m     x, attention_cem \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# added for DUST #TRIAL FOR CEM\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, attention_cem\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/rathna/New Volume/rathna_code/word_piece/conf_model.py:311\u001b[0m, in \u001b[0;36mConformerBlock.forward\u001b[0;34m(self, x, mask, p)\u001b[0m\n\u001b[1;32m    309\u001b[0m attention_cem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(x, mask\u001b[38;5;241m=\u001b[39mmask, p\u001b[38;5;241m=\u001b[39mp) \u001b[38;5;66;03m#TRIAL FOR CEM\u001b[39;00m\n\u001b[1;32m    310\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(x, mask\u001b[38;5;241m=\u001b[39mmask, p\u001b[38;5;241m=\u001b[39mp)           \u001b[38;5;66;03m# added for DUST\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m                     \u001b[38;5;66;03m# added for DUST\u001b[39;00m\n\u001b[1;32m    312\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff2(x, p\u001b[38;5;241m=\u001b[39mp))   \u001b[38;5;66;03m# added for DUST\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(x), attention_cem\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/rathna/New Volume/rathna_code/word_piece/conf_model.py:172\u001b[0m, in \u001b[0;36mConvBlock.forward\u001b[0;34m(self, x, p)\u001b[0m\n\u001b[1;32m    170\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# (batch_size, d_model, seq_len)\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m#x = self.module(x)\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglu(x)\n\u001b[1;32m    174\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d_2(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_gpu/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_gpu/lib/python3.9/site-packages/torch/nn/modules/conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_gpu/lib/python3.9/site-packages/torch/nn/modules/conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    307\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    308\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "best_wer = float('inf')\n",
    "optimizer.zero_grad()\n",
    "\n",
    "use_amp = True\n",
    "variational_noise_std = 0.0001\n",
    "\n",
    "# Mixed Precision Setup\n",
    "if use_amp:\n",
    "    print('Using Mixed Precision')\n",
    "grad_scaler = GradScaler(enabled=use_amp)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    print(f\"Epoch : {epoch+1}/{EPOCHS}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    #variational noise for regularization - COMMENTING ON MAY 19\n",
    "#     add_model_noise(encoder, std=variational_noise_std, gpu=True)\n",
    "#     add_model_noise(decoder, std=variational_noise_std, gpu=True)\n",
    "\n",
    "    \n",
    "    # Train/Validation loops\n",
    "    #wer, cer, loss = train(encoder_parallel, decoder_parallel, char_decoder, optimizer, scheduler, criterion, trainloader, DEVICE) \n",
    "    wer, cer, loss = train(encoder, decoder, char_decoder, optimizer, scheduler, criterion, grad_scaler, trainloader, DEVICE) \n",
    "    valid_wer, valid_cer, valid_loss = validate(encoder, decoder, char_decoder, criterion, devloader, DEVICE)\n",
    "    print(f'Epoch {epoch} - Valid WER: {valid_wer}%, Valid CER: {valid_cer}%, Valid Loss: {valid_loss}, Train WER: {wer}%, Train CER: {cer}%, Train Loss: {loss}')  \n",
    "    \n",
    "    # Save best model\n",
    "    if valid_loss <= best_loss:\n",
    "        print('Validation loss improved, saving best model.')\n",
    "        best_loss = valid_loss\n",
    "        save_checkpoint(encoder, decoder, optimizer, scheduler, valid_loss, epoch+1, best_loss_save_path, valid_wer, valid_cer, vocab_dict)\n",
    "    \n",
    "    if epoch%3==0:\n",
    "        print(f'Saving checkpoint at epoch:{epoch+1}')\n",
    "        save_checkpoint(encoder, decoder, optimizer, scheduler, valid_loss, epoch+1, checkpoint_save_path, valid_wer, valid_cer, vocab_dict)\n",
    "    \n",
    "    if valid_wer <= best_wer:\n",
    "        print(f\"Validation WER improved, saving best model.\")\n",
    "        best_wer = valid_wer\n",
    "        save_checkpoint(encoder, decoder, optimizer, scheduler, valid_loss, epoch+1, best_wer_save_path, valid_wer, valid_cer, vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169a49da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
