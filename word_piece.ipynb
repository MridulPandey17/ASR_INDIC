{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cce2cdb",
   "metadata": {},
   "source": [
    "RUNS IN NEMO_GPU and torch_gpu ENVIRONMENT\n",
    "Reference: https://colab.research.google.com/github/NVIDIA/NeMo/blob/r1.0.0rc1/tutorials/asr/08_ASR_with_Subword_Tokenization.ipynb\n",
    "https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt\n",
    "To install:\n",
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e1114b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The script above takes a few important arguments -\n",
    "\n",
    "    either --manifest or --data_file: If your text data lies inside of an ASR manifest file, then use the --manifest path. If instead the text data is inside a file with separate lines corresponding to different text lines, then use --data_file. In either case, you can add commas to concatenate different manifests or different data files.\n",
    "\n",
    "    --data_root: The output directory (whose subdirectories will be created if not present) where the tokenizers will be placed.\n",
    "\n",
    "    --vocab_size: The size of the tokenizer vocabulary. Larger vocabularies can accommodate almost entire words, but the decoder size of any model will grow proportionally.\n",
    "\n",
    "    --tokenizer: Can be either spe or wpe . spe refers to the Google sentencepiece library tokenizer. wpe refers to the HuggingFace BERT Word Piece tokenizer. Please refer to the papers above for the relevant technique in order to select an appropriate tokenizer.\n",
    "\n",
    "    --no_lower_case: When this flag is passed, it will force the tokenizer to create separate tokens for upper and lower case characters. By default, the script will turn all the text to lower case before tokenization (and if upper case characters are passed during training/inference, the tokenizer will emit a token equivalent to Out-Of-Vocabulary). Used primarily for the English language.\n",
    "\n",
    "    --spe_type: The sentencepiece library has a few implementations of the tokenization technique, and spe_type refers to these implementations. Currently supported types are unigram, bpe, char, word. Defaults to bpe.\n",
    "\n",
    "    --spe_character_coverage: The sentencepiece library considers how much of the original vocabulary it should cover in its \"base set\" of tokens (akin to the lower and upper case characters of the English language). For almost all languages with small base token sets (<1000 tokens), this should be kept at its default of 1.0. For languages with larger vocabularies (say Japanese, Mandarin, Korean etc), the suggested value is 0.9995.\n",
    "\n",
    "    --spe_sample_size: If the dataset is too large, consider using a sampled dataset indicated by a positive integer. By default, any negative value (default = -1) will use the entire dataset.\n",
    "\n",
    "    --spe_train_extremely_large_corpus: When training a sentencepiece tokenizer on very large amounts of text, sometimes the tokenizer will run out of memory or wont be able to process so much data on RAM. At some point you might receive the following error - \"Input corpus too large, try with train_extremely_large_corpus=true\". If your machine has large amounts of RAM, it might still be possible to build the tokenizer using the above flag. Will silently fail if it runs out of RAM.\n",
    "\n",
    "    --log: Whether the script should display log messages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a04c5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c90545c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/media/rathna/New Volume/word_piece/tokenizer_wpe_v1000/vocab.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 1000 #CAN SPECIFY ANY MAXIMUM VALUE\n",
    "tokenizer_type = \"wpe\"\n",
    "dst_folder = \"/media/rathna/New Volume/word_piece\"\n",
    "text_path = \"/media/rathna/New Volume/word_piece/libri_text.txt\"\n",
    "\n",
    "tokenizer_dir = os.path.join(dst_folder, 'tokenizer_{}_v{}').format(tokenizer_type, vocab_size)\n",
    "\n",
    "if not os.path.exists(tokenizer_dir):\n",
    "    os.makedirs(tokenizer_dir)\n",
    "\n",
    "tokenizer = tokenizers.BertWordPieceTokenizer(lowercase=False) #if true, treats upper and lower case as separate tokens\n",
    "\n",
    "tokenizer.train(text_path, vocab_size=vocab_size)\n",
    "tokenizer.save_model(tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cf3c615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '##i', '##l', '##a', '##r', '##e', '##d', '##m', '##n', '##o', '##u', '##s', '##p', '##t', '##z', '##c', '##h', '##b', '##k', '##x', '##v', '##g', '##y', '##w', '##f', '##j', '##q', 'th', 'the', '##er', '##nd', '##in', '##ed', '##ou', '##at', '##en', 'and', '##es', 'to', '##or', 'of', '##on', '##is', '##ing', '##ar', '##as', '##an', '##it', '##ll', 'in', '##re', 'wh', '##om', 'he', 'ha', 'be', '##le', '##ic', '##ot', '##ow', 'was', '##ut', 'it', '##ld', 'that', '##ly', 'sh', '##gh', '##se', '##id', 'on', '##ve', '##ent', '##et', '##im', 'you', '##ion', '##ir', '##ce', '##st', 'as', '##ith', 'for', 'his', '##ay', '##al', '##ur', '##ter', 'with', 'st', '##ch', '##ver', 'her', 're', 'had', '##ad', '##ght', 'an', 'not', '##am', '##her', 'at', 'is', '##ess', '##oo', '##ould', 'but', '##ct', 'fr', 'she', 'se', 'we', 'pr', 'sa', '##ere', 'him', 'so', '##ill', 'su', '##ain', 'me', '##ight', '##il', '##all', 'my', '##ke', '##est', 'they', 'ch', '##ich', 'de', 'all', '##pp', 'ne', '##us', '##ck', '##ome', 'by', 'li', 'have', 'were', '##ri', 'con', 'this', 'which', '##ore', '##th', '##ul', '##ra', '##ard', 'from', 'do', 'up', '##ge', 'ex', 'said', '##ant', '##nt', 'one', '##os', 'or', 'no', '##ble', '##ation', 'who', '##ers', '##pe', '##ous', 'al', 'man', 'there', 'fe', '##and', '##ie', '##ame', 'them', '##ust', 'mis', 'when', 'their', '##out', '##ast', '##art', '##lf', '##em', '##ind', '##our', '##ood', 'le', 'would', 'com', '##ong', '##ound', 'kn', '##ate', '##if', 'ab', '##ist', '##ol', 'en', 'int', '##op', '##ough', '##ap', 'if', 'go', '##ak', 'what', 'out', '##ish', 'br', '##um', 'wor', 'are', 'been', 'un', 'ar', '##un', '##itt', '##ig', '##one', '##la', '##own', 'af', '##ook', '##ive', 'will', '##self', 'then', '##reat', '##ort', 'fa', '##ven', 'could', '##ity', '##ery', 'tr', '##el', 'ro', 'tw', 'ag', 'sp', 'im', '##ost', 'qu', 'some', 'ever', '##ep', '##ther', '##ake', '##nder', '##ure', 'am', 'more', 'any', '##au', 'into', '##ved', 'tim', '##ence', 'did', '##ose', 'your', '##ag', '##fe', '##ry', 'gr', 'ad', 'other', '##qu', 'than', 'bo', '##ance', '##ach', 'very', '##ear', '##cc', 'its', '##way', '##ies', 'cl', 'now', '##ine', '##ass', '##able', 'us', '##ittle', 'like', 'little', '##ack', '##ade', '##other', 'lo', '##ab', 'can', '##hing', 'co', 'day', 'about', '##ought', '##ite', 'dis', 'look', '##urn', 'how', '##ci', '##ked', 'car', 'after', 'time', 'over', '##wn', 'well', '##ice', 'pro', '##pt', '##fore', 'know', '##ment', '##ell', 'again', '##ide', '##ree', 'per', '##ather', 'too', '##ally', '##ro', 'po', 'upon', '##ving', 'see', 'has', '##pl', 'great', 'our', 'hand', 'fl', 'only', '##oss', '##ud', 'two', '##ings', '##ied', 'say', '##ont', '##ink', 'str', 'every', 'mar', '##ty', '##ount', 'app', 'pe', '##ile', 'should', 'down', 'thr', 'made', 'pl', 'miss', 'before', '##age', '##ire', 'old', 'cr', 'off', '##ick', 'good', '##ions', 'long', '##ress', '##ward', '##ue', '##ft', '##ves', '##ang', 'bl', 'such', 'mu', 'these', 'fir', '##een', '##tain', 'way', '##ful', 'even', 'pre', '##ull', 'ind', '##ated', '##ered', 'came', '##led', '##ild', 'bec', 'under', 'never', 'sm', 'mister', '##ans', '##ness', 'much', 'part', 'ey', 'back', '##ious', 'where', 'rem', 'must', 'res', '##orn', 'fo', 'sc', '##ouse', '##ces', '##ff', '##ber', '##ect', 'just', 'cont', 'ob', 'gl', 'bet', 'mo', '##int', 'come', '##ath', 'thought', '##ced', 'spe', 'own', 'think', '##ord', 'des', 'may', 'first', 'pla', 'men', 'seem', 'ho', 'imp', '##ried', 'comp', 'might', '##ary', '##ily', '##co', 'himself', 'beg', '##iz', '##ise', 'house', 'let', '##ip', 'through', 'op', '##ng', 'jo', 'went', '##ons', 'here', 'bel', '##cl', 'sw', 'get', 'new', '##ady', 'em', 'head', '##ens', 'pass', '##igh', '##omet', 'att', '##ened', 'pres', 'eyes', '##iv', '##ors', 'make', '##ater', 'most', 'without', '##llow', 'those', 'somet', 'many', '##ave', 'life', 'away', 'wom', '##red', 'far', 'call', 'acc', 'turn', '##ew', 'young', 'cons', '##per', 'being', '##ys', '##oth', '##iff', 'still', 'inter', '##ign', 'last', '##ory', 'night', 'ear', '##ac', '##ause', 'found', 'comm', 'nothing', 'tell', 'while', 'ret', 'dont', '##ark', '##od', 'peop', 'year', '##ible', 'ser', '##ase', 'though', 'gra', '##ether', '##ian', '##ition', 'sl', '##aking', 'yet', 'right', '##ction', 'word', 'people', 'three', '##atter', '##irl', '##les', '##ne', '##ents', '##alk', '##pect', '##gg', '##ently', '##ult', 'put', 'room', 'take', 'saw', 'happ', '##ix', 'count', 'asked', 'hu', '##end', '##ict', 'rep', 'child', 'same', '##less', '##air', 'missus', 'want', 'nat', '##ub', 'work', '##are', 'heart', '##ways', 'inst', '##med', 'another', 'ac', '##thing', 'shall', 'left', 'la', 'wr', 'father', '##ations', 'cour', 'dist', 'girl', 'end', 'nor', '##ished', 'once', 'char', 'place', '##augh', 'pers', 'took', 'face', 'seemed', '##iend', 'near', 'always', '##atch', '##ace', 'supp', '##ject', 'gen', 'cap', 'gre', 'mom', 'love', 'ple', 'friend', 'why', 'told', '##ank', 'arm', 'el', 'eng', '##cess', '##ters', 'mind', 'war', '##ling', '##ower', 'prin', 'looked', 'te', 'heard', '##aut', '##vent', '##ange', '##der', 'soon', '##pped', '##sel', '##iver', '##lt', '##ertain', 'set', '##ever', '##sw', 'sat', 'poss', 'mother', 'light', '##amp', 'because', 'sur', 'vo', '##fect', 'things', 'answ', 'yes', 'something', '##ual', '##land', '##ia', 'diff', '##uck', '##ars', 'going', 'give', '##ret', 'sub', '##ail', 'id', 'feel', 'find', 'ill', 'rel', 'ass', 'reg', 'world', 'tra', 'rest', '##ock', 'door', 'high', '##man', 'gu', 'inf', 'wind', 'gi', 'dre', 'home', 'wa', 'certain', '##ical', 'unc', 'ph', 'return', '##av', 'hands', 'belie', 'king', 'thing', 'ke', '##den', 'moment', 'exp', 'appear', 'knew', 'ma', '##ial', '##ush', 'bar', '##ott', 'prop', 'each', 'bre', 'against', 'having', '##cept', 'few', 'wo', 'hard', 'beh', '##ined', '##ither', 'years', 'cle', 'sir', 'began', 'pur', 'kind', '##orm', '##aps', 'better', 'sil', 'side', '##ited', '##ru', 'unt', '##ning', '##ool', 'follow', 'enough', 'hum', 'eight', 'del', 'four', 'occ', '##ner', '##ared', '##iness', 'conf', 'tre', '##ple', 'woman', 'sn', 'gener', 'par', '##ph', 'done', 'got', 'morn', 'betw', '##ting', 'seen', 'ev', '##gether', 'min', 'ref', '##cy', '##ash', 'whole', 'dra', 'add', 'called', 'mon', '##aken', '##ully', '##ted', 'beaut', 'sin', '##ndred', 'hundred', 'bu', '##owed', '##ures', 'between', '##act', 'inc', '##ying', '##most', '##so', '##ating', 'ins', '##ired', 'felt', 'open', 'hour', '##ular', '##oub', 'ter', 'morning', 'water', '##res', '##ows', 'pleas', 'toward', '##ained', 'white', '##plied', 'act', '##ier', '##ised', 'hel', 'form', 'herself', 'exc', '##osed', '##ung', 'oh', 'six', 'land', '##oke', 'half', 'inde', '##ps', 'among', 'however', 'turned', 'stre', '##haps', 'name', '##uth', 'days', 'bla', '##tered', 'mas', '##ks', 'ste', 'also', 'poor', '##ib', 'perhaps', 'fam', 'replied', '##rib', 'cor', 'sle', 'does', 'care', 'point', 'quite', 'myself', 'adm', 'dr', '##selves', 'sim', '##ched', 'cur', 'matter', 'adv', 'course', 'keep', 'fall', 'pat', 'sk', '##ins', 'voice', '##ale', '##ates', '##eter', '##xt', 'small', 'wonder', 'both', 'sent', 'gave', 'es', '##up', 'sure', 'col', '##ond', 'whom', '##enty', '##iet', '##ason', 'ext', 'wish', 'almost', 'hor', 'less', '##its', 'contin', 'aw', '##vel', 'god', 'sy', '##not', '##cei', 'thir', 'best', 'mer', '##oud', '##eng', 'stand', 'need', 'others', '##ute', '##te', 'together', 'mean', 'person', 'power', '##ately', 'words', 'ed', 'met', '##fort', 'lar', 'suff', 'sou', 'until', '##uring', '##hn', 'dear', 'lay', 'feet', 'anything', 'next', 'dark', '##ense', 'stood', 'present', 'laugh', '##oney', 'black', 'brought', 'fact', 'che', 'underst', '##ged', '##als', 'red', 'vis', 'round', 'consid', 'wood', '##ove', 'along', 'five', 'read', '##dden', 'sun', 'play', '##ued', '##ured', 'bod', 'full', 'air', 'sudden', 'har', '##iled', '##ned', 'hear', '##used', 'looking', 'john', 'quest', 'reach', '##ten', '##ants', 'serv', 'ent', 'rather', '']\n"
     ]
    }
   ],
   "source": [
    "# opening the file in read mode\n",
    "my_file = open(\"/media/rathna/New Volume/word_piece/tokenizer_wpe_v1000/vocab.txt\", \"r\")\n",
    "  \n",
    "# reading the file\n",
    "data = my_file.read()\n",
    "  \n",
    "# replacing end splitting the text \n",
    "# when newline ('\\n') is seen.\n",
    "vocab = data.split(\"\\n\")\n",
    "print(vocab)\n",
    "my_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21a698ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word):\n",
    "    tokens = []\n",
    "    while len(word) > 0:\n",
    "        i = len(word)\n",
    "        while i > 0 and word[:i] not in vocab:\n",
    "            i -= 1\n",
    "        if i == 0:\n",
    "            return [\"[UNK]\"]\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e06d103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cl', '##osed']\n",
      "['open', '##ed']\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"closed\"))\n",
    "print(encode_word(\"opened\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b738e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    print(pre_tokenize_result)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    print(pre_tokenized_text)\n",
    "    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "731a2540",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chapter', (0, 7)), ('one', (8, 11)), ('missus', (12, 18)), ('rachel', (19, 25)), ('lynde', (26, 31)), ('is', (32, 34)), ('surprised', (35, 44)), ('missus', (45, 51)), ('rachel', (52, 58)), ('lynde', (59, 64)), ('lived', (65, 70)), ('just', (71, 75)), ('where', (76, 81)), ('the', (82, 85)), ('avonlea', (86, 93)), ('main', (94, 98)), ('road', (99, 103)), ('dipped', (104, 110)), ('down', (111, 115)), ('into', (116, 120)), ('a', (121, 122)), ('little', (123, 129)), ('hollow', (130, 136)), ('fringed', (137, 144)), ('with', (145, 149)), ('alders', (150, 156)), ('and', (157, 160)), ('ladies', (161, 167)), ('eardrops', (168, 176)), ('and', (177, 180)), ('traversed', (181, 190)), ('by', (191, 193)), ('a', (194, 195)), ('brook', (196, 201))]\n",
      "['chapter', 'one', 'missus', 'rachel', 'lynde', 'is', 'surprised', 'missus', 'rachel', 'lynde', 'lived', 'just', 'where', 'the', 'avonlea', 'main', 'road', 'dipped', 'down', 'into', 'a', 'little', 'hollow', 'fringed', 'with', 'alders', 'and', 'ladies', 'eardrops', 'and', 'traversed', 'by', 'a', 'brook']\n",
      "['ch', '##ap', '##ter', 'one', 'missus', 'r', '##ach', '##el', 'l', '##y', '##nd', '##e', 'is', 'sur', '##p', '##ri', '##se', '##d', 'missus', 'r', '##ach', '##el', 'l', '##y', '##nd', '##e', 'li', '##ved', 'just', 'where', 'the', 'a', '##v', '##on', '##le', '##a', 'ma', '##in', 'ro', '##ad', 'd', '##ip', '##pe', '##d', 'down', 'into', 'a', 'little', 'ho', '##llow', 'fr', '##ing', '##ed', 'with', 'al', '##der', '##s', 'and', 'la', '##d', '##ies', 'ear', '##d', '##ro', '##ps', 'and', 'tra', '##ver', '##se', '##d', 'by', 'a', 'br', '##ook']\n"
     ]
    }
   ],
   "source": [
    "text = \"chapter one missus rachel lynde is surprised missus rachel lynde lived just where the avonlea main road dipped down into a little hollow fringed with alders and ladies eardrops and traversed by a brook\"\n",
    "print(tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "250677c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "995\n",
      "996\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '##i', '##l', '##a', '##r', '##e', '##d', '##m', '##n', '##o', '##u', '##s', '##p', '##t', '##z', '##c', '##h', '##b', '##k', '##x', '##v', '##g', '##y', '##w', '##f', '##j', '##q', 'th', 'the', '##er', '##nd', '##in', '##ed', '##ou', '##at', '##en', 'and', '##es', 'to', '##or', 'of', '##on', '##is', '##ing', '##ar', '##as', '##an', '##it', '##ll', 'in', '##re', 'wh', '##om', 'he', 'ha', 'be', '##le', '##ic', '##ot', '##ow', 'was', '##ut', 'it', '##ld', 'that', '##ly', 'sh', '##gh', '##se', '##id', 'on', '##ve', '##ent', '##et', '##im', 'you', '##ion', '##ir', '##ce', '##st', 'as', '##ith', 'for', 'his', '##ay', '##al', '##ur', '##ter', 'with', 'st', '##ch', '##ver', 'her', 're', 'had', '##ad', '##ght', 'an', 'not', '##am', '##her', 'at', 'is', '##ess', '##oo', '##ould', 'but', '##ct', 'fr', 'she', 'se', 'we', 'pr', 'sa', '##ere', 'him', 'so', '##ill', 'su', '##ain', 'me', '##ight', '##il', '##all', 'my', '##ke', '##est', 'they', 'ch', '##ich', 'de', 'all', '##pp', 'ne', '##us', '##ck', '##ome', 'by', 'li', 'have', 'were', '##ri', 'con', 'this', 'which', '##ore', '##th', '##ul', '##ra', '##ard', 'from', 'do', 'up', '##ge', 'ex', 'said', '##ant', '##nt', 'one', '##os', 'or', 'no', '##ble', '##ation', 'who', '##ers', '##pe', '##ous', 'al', 'man', 'there', 'fe', '##and', '##ie', '##ame', 'them', '##ust', 'mis', 'when', 'their', '##out', '##ast', '##art', '##lf', '##em', '##ind', '##our', '##ood', 'le', 'would', 'com', '##ong', '##ound', 'kn', '##ate', '##if', 'ab', '##ist', '##ol', 'en', 'int', '##op', '##ough', '##ap', 'if', 'go', '##ak', 'what', 'out', '##ish', 'br', '##um', 'wor', 'are', 'been', 'un', 'ar', '##un', '##itt', '##ig', '##one', '##la', '##own', 'af', '##ook', '##ive', 'will', '##self', 'then', '##reat', '##ort', 'fa', '##ven', 'could', '##ity', '##ery', 'tr', '##el', 'ro', 'tw', 'ag', 'sp', 'im', '##ost', 'qu', 'some', 'ever', '##ep', '##ther', '##ake', '##nder', '##ure', 'am', 'more', 'any', '##au', 'into', '##ved', 'tim', '##ence', 'did', '##ose', 'your', '##ag', '##fe', '##ry', 'gr', 'ad', 'other', '##qu', 'than', 'bo', '##ance', '##ach', 'very', '##ear', '##cc', 'its', '##way', '##ies', 'cl', 'now', '##ine', '##ass', '##able', 'us', '##ittle', 'like', 'little', '##ack', '##ade', '##other', 'lo', '##ab', 'can', '##hing', 'co', 'day', 'about', '##ought', '##ite', 'dis', 'look', '##urn', 'how', '##ci', '##ked', 'car', 'after', 'time', 'over', '##wn', 'well', '##ice', 'pro', '##pt', '##fore', 'know', '##ment', '##ell', 'again', '##ide', '##ree', 'per', '##ather', 'too', '##ally', '##ro', 'po', 'upon', '##ving', 'see', 'has', '##pl', 'great', 'our', 'hand', 'fl', 'only', '##oss', '##ud', 'two', '##ings', '##ied', 'say', '##ont', '##ink', 'str', 'every', 'mar', '##ty', '##ount', 'app', 'pe', '##ile', 'should', 'down', 'thr', 'made', 'pl', 'miss', 'before', '##age', '##ire', 'old', 'cr', 'off', '##ick', 'good', '##ions', 'long', '##ress', '##ward', '##ue', '##ft', '##ves', '##ang', 'bl', 'such', 'mu', 'these', 'fir', '##een', '##tain', 'way', '##ful', 'even', 'pre', '##ull', 'ind', '##ated', '##ered', 'came', '##led', '##ild', 'bec', 'under', 'never', 'sm', 'mister', '##ans', '##ness', 'much', 'part', 'ey', 'back', '##ious', 'where', 'rem', 'must', 'res', '##orn', 'fo', 'sc', '##ouse', '##ces', '##ff', '##ber', '##ect', 'just', 'cont', 'ob', 'gl', 'bet', 'mo', '##int', 'come', '##ath', 'thought', '##ced', 'spe', 'own', 'think', '##ord', 'des', 'may', 'first', 'pla', 'men', 'seem', 'ho', 'imp', '##ried', 'comp', 'might', '##ary', '##ily', '##co', 'himself', 'beg', '##iz', '##ise', 'house', 'let', '##ip', 'through', 'op', '##ng', 'jo', 'went', '##ons', 'here', 'bel', '##cl', 'sw', 'get', 'new', '##ady', 'em', 'head', '##ens', 'pass', '##igh', '##omet', 'att', '##ened', 'pres', 'eyes', '##iv', '##ors', 'make', '##ater', 'most', 'without', '##llow', 'those', 'somet', 'many', '##ave', 'life', 'away', 'wom', '##red', 'far', 'call', 'acc', 'turn', '##ew', 'young', 'cons', '##per', 'being', '##ys', '##oth', '##iff', 'still', 'inter', '##ign', 'last', '##ory', 'night', 'ear', '##ac', '##ause', 'found', 'comm', 'nothing', 'tell', 'while', 'ret', 'dont', '##ark', '##od', 'peop', 'year', '##ible', 'ser', '##ase', 'though', 'gra', '##ether', '##ian', '##ition', 'sl', '##aking', 'yet', 'right', '##ction', 'word', 'people', 'three', '##atter', '##irl', '##les', '##ne', '##ents', '##alk', '##pect', '##gg', '##ently', '##ult', 'put', 'room', 'take', 'saw', 'happ', '##ix', 'count', 'asked', 'hu', '##end', '##ict', 'rep', 'child', 'same', '##less', '##air', 'missus', 'want', 'nat', '##ub', 'work', '##are', 'heart', '##ways', 'inst', '##med', 'another', 'ac', '##thing', 'shall', 'left', 'la', 'wr', 'father', '##ations', 'cour', 'dist', 'girl', 'end', 'nor', '##ished', 'once', 'char', 'place', '##augh', 'pers', 'took', 'face', 'seemed', '##iend', 'near', 'always', '##atch', '##ace', 'supp', '##ject', 'gen', 'cap', 'gre', 'mom', 'love', 'ple', 'friend', 'why', 'told', '##ank', 'arm', 'el', 'eng', '##cess', '##ters', 'mind', 'war', '##ling', '##ower', 'prin', 'looked', 'te', 'heard', '##aut', '##vent', '##ange', '##der', 'soon', '##pped', '##sel', '##iver', '##lt', '##ertain', 'set', '##ever', '##sw', 'sat', 'poss', 'mother', 'light', '##amp', 'because', 'sur', 'vo', '##fect', 'things', 'answ', 'yes', 'something', '##ual', '##land', '##ia', 'diff', '##uck', '##ars', 'going', 'give', '##ret', 'sub', '##ail', 'id', 'feel', 'find', 'ill', 'rel', 'ass', 'reg', 'world', 'tra', 'rest', '##ock', 'door', 'high', '##man', 'gu', 'inf', 'wind', 'gi', 'dre', 'home', 'wa', 'certain', '##ical', 'unc', 'ph', 'return', '##av', 'hands', 'belie', 'king', 'thing', 'ke', '##den', 'moment', 'exp', 'appear', 'knew', 'ma', '##ial', '##ush', 'bar', '##ott', 'prop', 'each', 'bre', 'against', 'having', '##cept', 'few', 'wo', 'hard', 'beh', '##ined', '##ither', 'years', 'cle', 'sir', 'began', 'pur', 'kind', '##orm', '##aps', 'better', 'sil', 'side', '##ited', '##ru', 'unt', '##ning', '##ool', 'follow', 'enough', 'hum', 'eight', 'del', 'four', 'occ', '##ner', '##ared', '##iness', 'conf', 'tre', '##ple', 'woman', 'sn', 'gener', 'par', '##ph', 'done', 'got', 'morn', 'betw', '##ting', 'seen', 'ev', '##gether', 'min', 'ref', '##cy', '##ash', 'whole', 'dra', 'add', 'called', 'mon', '##aken', '##ully', '##ted', 'beaut', 'sin', '##ndred', 'hundred', 'bu', '##owed', '##ures', 'between', '##act', 'inc', '##ying', '##most', '##so', '##ating', 'ins', '##ired', 'felt', 'open', 'hour', '##ular', '##oub', 'ter', 'morning', 'water', '##res', '##ows', 'pleas', 'toward', '##ained', 'white', '##plied', 'act', '##ier', '##ised', 'hel', 'form', 'herself', 'exc', '##osed', '##ung', 'oh', 'six', 'land', '##oke', 'half', 'inde', '##ps', 'among', 'however', 'turned', 'stre', '##haps', 'name', '##uth', 'days', 'bla', '##tered', 'mas', '##ks', 'ste', 'also', 'poor', '##ib', 'perhaps', 'fam', 'replied', '##rib', 'cor', 'sle', 'does', 'care', 'point', 'quite', 'myself', 'adm', 'dr', '##selves', 'sim', '##ched', 'cur', 'matter', 'adv', 'course', 'keep', 'fall', 'pat', 'sk', '##ins', 'voice', '##ale', '##ates', '##eter', '##xt', 'small', 'wonder', 'both', 'sent', 'gave', 'es', '##up', 'sure', 'col', '##ond', 'whom', '##enty', '##iet', '##ason', 'ext', 'wish', 'almost', 'hor', 'less', '##its', 'contin', 'aw', '##vel', 'god', 'sy', '##not', '##cei', 'thir', 'best', 'mer', '##oud', '##eng', 'stand', 'need', 'others', '##ute', '##te', 'together', 'mean', 'person', 'power', '##ately', 'words', 'ed', 'met', '##fort', 'lar', 'suff', 'sou', 'until', '##uring', '##hn', 'dear', 'lay', 'feet', 'anything', 'next', 'dark', '##ense', 'stood', 'present', 'laugh', '##oney', 'black', 'brought', 'fact', 'che', 'underst', '##ged', '##als', 'red', 'vis', 'round', 'consid', 'wood', '##ove', 'along', 'five', 'read', '##dden', 'sun', 'play', '##ued', '##ured', 'bod', 'full', 'air', 'sudden', 'har', '##iled', '##ned', 'hear', '##used', 'looking', 'john', 'quest', 'reach', '##ten', '##ants', 'serv', 'ent', 'rather', ' ']\n"
     ]
    }
   ],
   "source": [
    "vocab = vocab[5:-1]\n",
    "print(len(vocab))\n",
    "vocab = vocab+[' '] # Add apostrophe later\n",
    "print(len(vocab))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a3515bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "996\n",
      "Character mappings : {' ': 0, '##a': 1, '##ab': 2, '##able': 3, '##ac': 4, '##ace': 5, '##ach': 6, '##ack': 7, '##act': 8, '##ad': 9, '##ade': 10, '##ady': 11, '##ag': 12, '##age': 13, '##ail': 14, '##ain': 15, '##ained': 16, '##air': 17, '##ak': 18, '##ake': 19, '##aken': 20, '##aking': 21, '##al': 22, '##ale': 23, '##alk': 24, '##all': 25, '##ally': 26, '##als': 27, '##am': 28, '##ame': 29, '##amp': 30, '##an': 31, '##ance': 32, '##and': 33, '##ang': 34, '##ange': 35, '##ank': 36, '##ans': 37, '##ant': 38, '##ants': 39, '##ap': 40, '##aps': 41, '##ar': 42, '##ard': 43, '##are': 44, '##ared': 45, '##ark': 46, '##ars': 47, '##art': 48, '##ary': 49, '##as': 50, '##ase': 51, '##ash': 52, '##ason': 53, '##ass': 54, '##ast': 55, '##at': 56, '##atch': 57, '##ate': 58, '##ated': 59, '##ately': 60, '##ater': 61, '##ates': 62, '##ath': 63, '##ather': 64, '##ating': 65, '##ation': 66, '##ations': 67, '##atter': 68, '##au': 69, '##augh': 70, '##ause': 71, '##aut': 72, '##av': 73, '##ave': 74, '##ay': 75, '##b': 76, '##ber': 77, '##ble': 78, '##c': 79, '##cc': 80, '##ce': 81, '##ced': 82, '##cei': 83, '##cept': 84, '##ces': 85, '##cess': 86, '##ch': 87, '##ched': 88, '##ci': 89, '##ck': 90, '##cl': 91, '##co': 92, '##ct': 93, '##ction': 94, '##cy': 95, '##d': 96, '##dden': 97, '##den': 98, '##der': 99, '##e': 100, '##ear': 101, '##ect': 102, '##ed': 103, '##een': 104, '##el': 105, '##ell': 106, '##em': 107, '##en': 108, '##ence': 109, '##end': 110, '##ened': 111, '##eng': 112, '##ens': 113, '##ense': 114, '##ent': 115, '##ently': 116, '##ents': 117, '##enty': 118, '##ep': 119, '##er': 120, '##ere': 121, '##ered': 122, '##ers': 123, '##ertain': 124, '##ery': 125, '##es': 126, '##ess': 127, '##est': 128, '##et': 129, '##eter': 130, '##ether': 131, '##ever': 132, '##ew': 133, '##f': 134, '##fe': 135, '##fect': 136, '##ff': 137, '##fore': 138, '##fort': 139, '##ft': 140, '##ful': 141, '##g': 142, '##ge': 143, '##ged': 144, '##gether': 145, '##gg': 146, '##gh': 147, '##ght': 148, '##h': 149, '##haps': 150, '##her': 151, '##hing': 152, '##hn': 153, '##i': 154, '##ia': 155, '##ial': 156, '##ian': 157, '##ib': 158, '##ible': 159, '##ic': 160, '##ical': 161, '##ice': 162, '##ich': 163, '##ick': 164, '##ict': 165, '##id': 166, '##ide': 167, '##ie': 168, '##ied': 169, '##iend': 170, '##ier': 171, '##ies': 172, '##iet': 173, '##if': 174, '##iff': 175, '##ig': 176, '##igh': 177, '##ight': 178, '##ign': 179, '##il': 180, '##ild': 181, '##ile': 182, '##iled': 183, '##ill': 184, '##ily': 185, '##im': 186, '##in': 187, '##ind': 188, '##ine': 189, '##ined': 190, '##iness': 191, '##ing': 192, '##ings': 193, '##ink': 194, '##ins': 195, '##int': 196, '##ion': 197, '##ions': 198, '##ious': 199, '##ip': 200, '##ir': 201, '##ire': 202, '##ired': 203, '##irl': 204, '##is': 205, '##ise': 206, '##ised': 207, '##ish': 208, '##ished': 209, '##ist': 210, '##it': 211, '##ite': 212, '##ited': 213, '##ith': 214, '##ither': 215, '##ition': 216, '##its': 217, '##itt': 218, '##ittle': 219, '##ity': 220, '##iv': 221, '##ive': 222, '##iver': 223, '##ix': 224, '##iz': 225, '##j': 226, '##ject': 227, '##k': 228, '##ke': 229, '##ked': 230, '##ks': 231, '##l': 232, '##la': 233, '##land': 234, '##ld': 235, '##le': 236, '##led': 237, '##les': 238, '##less': 239, '##lf': 240, '##ling': 241, '##ll': 242, '##llow': 243, '##lt': 244, '##ly': 245, '##m': 246, '##man': 247, '##med': 248, '##ment': 249, '##most': 250, '##n': 251, '##nd': 252, '##nder': 253, '##ndred': 254, '##ne': 255, '##ned': 256, '##ner': 257, '##ness': 258, '##ng': 259, '##ning': 260, '##not': 261, '##nt': 262, '##o': 263, '##ock': 264, '##od': 265, '##oke': 266, '##ol': 267, '##om': 268, '##ome': 269, '##omet': 270, '##on': 271, '##ond': 272, '##one': 273, '##oney': 274, '##ong': 275, '##ons': 276, '##ont': 277, '##oo': 278, '##ood': 279, '##ook': 280, '##ool': 281, '##op': 282, '##or': 283, '##ord': 284, '##ore': 285, '##orm': 286, '##orn': 287, '##ors': 288, '##ort': 289, '##ory': 290, '##os': 291, '##ose': 292, '##osed': 293, '##oss': 294, '##ost': 295, '##ot': 296, '##oth': 297, '##other': 298, '##ott': 299, '##ou': 300, '##oub': 301, '##oud': 302, '##ough': 303, '##ought': 304, '##ould': 305, '##ound': 306, '##ount': 307, '##our': 308, '##ous': 309, '##ouse': 310, '##out': 311, '##ove': 312, '##ow': 313, '##owed': 314, '##ower': 315, '##own': 316, '##ows': 317, '##p': 318, '##pe': 319, '##pect': 320, '##per': 321, '##ph': 322, '##pl': 323, '##ple': 324, '##plied': 325, '##pp': 326, '##pped': 327, '##ps': 328, '##pt': 329, '##q': 330, '##qu': 331, '##r': 332, '##ra': 333, '##re': 334, '##reat': 335, '##red': 336, '##ree': 337, '##res': 338, '##ress': 339, '##ret': 340, '##ri': 341, '##rib': 342, '##ried': 343, '##ro': 344, '##ru': 345, '##ry': 346, '##s': 347, '##se': 348, '##sel': 349, '##self': 350, '##selves': 351, '##so': 352, '##st': 353, '##sw': 354, '##t': 355, '##tain': 356, '##te': 357, '##ted': 358, '##ten': 359, '##ter': 360, '##tered': 361, '##ters': 362, '##th': 363, '##ther': 364, '##thing': 365, '##ting': 366, '##ty': 367, '##u': 368, '##ual': 369, '##ub': 370, '##uck': 371, '##ud': 372, '##ue': 373, '##ued': 374, '##ul': 375, '##ular': 376, '##ull': 377, '##ully': 378, '##ult': 379, '##um': 380, '##un': 381, '##ung': 382, '##up': 383, '##ur': 384, '##ure': 385, '##ured': 386, '##ures': 387, '##uring': 388, '##urn': 389, '##us': 390, '##used': 391, '##ush': 392, '##ust': 393, '##ut': 394, '##ute': 395, '##uth': 396, '##v': 397, '##ve': 398, '##ved': 399, '##vel': 400, '##ven': 401, '##vent': 402, '##ver': 403, '##ves': 404, '##ving': 405, '##w': 406, '##ward': 407, '##way': 408, '##ways': 409, '##wn': 410, '##x': 411, '##xt': 412, '##y': 413, '##ying': 414, '##ys': 415, '##z': 416, 'a': 417, 'ab': 418, 'about': 419, 'ac': 420, 'acc': 421, 'act': 422, 'ad': 423, 'add': 424, 'adm': 425, 'adv': 426, 'af': 427, 'after': 428, 'ag': 429, 'again': 430, 'against': 431, 'air': 432, 'al': 433, 'all': 434, 'almost': 435, 'along': 436, 'also': 437, 'always': 438, 'am': 439, 'among': 440, 'an': 441, 'and': 442, 'another': 443, 'answ': 444, 'any': 445, 'anything': 446, 'app': 447, 'appear': 448, 'ar': 449, 'are': 450, 'arm': 451, 'as': 452, 'asked': 453, 'ass': 454, 'at': 455, 'att': 456, 'aw': 457, 'away': 458, 'b': 459, 'back': 460, 'bar': 461, 'be': 462, 'beaut': 463, 'bec': 464, 'because': 465, 'been': 466, 'before': 467, 'beg': 468, 'began': 469, 'beh': 470, 'being': 471, 'bel': 472, 'belie': 473, 'best': 474, 'bet': 475, 'better': 476, 'betw': 477, 'between': 478, 'bl': 479, 'bla': 480, 'black': 481, 'bo': 482, 'bod': 483, 'both': 484, 'br': 485, 'bre': 486, 'brought': 487, 'bu': 488, 'but': 489, 'by': 490, 'c': 491, 'call': 492, 'called': 493, 'came': 494, 'can': 495, 'cap': 496, 'car': 497, 'care': 498, 'certain': 499, 'ch': 500, 'char': 501, 'che': 502, 'child': 503, 'cl': 504, 'cle': 505, 'co': 506, 'col': 507, 'com': 508, 'come': 509, 'comm': 510, 'comp': 511, 'con': 512, 'conf': 513, 'cons': 514, 'consid': 515, 'cont': 516, 'contin': 517, 'cor': 518, 'could': 519, 'count': 520, 'cour': 521, 'course': 522, 'cr': 523, 'cur': 524, 'd': 525, 'dark': 526, 'day': 527, 'days': 528, 'de': 529, 'dear': 530, 'del': 531, 'des': 532, 'did': 533, 'diff': 534, 'dis': 535, 'dist': 536, 'do': 537, 'does': 538, 'done': 539, 'dont': 540, 'door': 541, 'down': 542, 'dr': 543, 'dra': 544, 'dre': 545, 'e': 546, 'each': 547, 'ear': 548, 'ed': 549, 'eight': 550, 'el': 551, 'em': 552, 'en': 553, 'end': 554, 'eng': 555, 'enough': 556, 'ent': 557, 'es': 558, 'ev': 559, 'even': 560, 'ever': 561, 'every': 562, 'ex': 563, 'exc': 564, 'exp': 565, 'ext': 566, 'ey': 567, 'eyes': 568, 'f': 569, 'fa': 570, 'face': 571, 'fact': 572, 'fall': 573, 'fam': 574, 'far': 575, 'father': 576, 'fe': 577, 'feel': 578, 'feet': 579, 'felt': 580, 'few': 581, 'find': 582, 'fir': 583, 'first': 584, 'five': 585, 'fl': 586, 'fo': 587, 'follow': 588, 'for': 589, 'form': 590, 'found': 591, 'four': 592, 'fr': 593, 'friend': 594, 'from': 595, 'full': 596, 'g': 597, 'gave': 598, 'gen': 599, 'gener': 600, 'get': 601, 'gi': 602, 'girl': 603, 'give': 604, 'gl': 605, 'go': 606, 'god': 607, 'going': 608, 'good': 609, 'got': 610, 'gr': 611, 'gra': 612, 'gre': 613, 'great': 614, 'gu': 615, 'h': 616, 'ha': 617, 'had': 618, 'half': 619, 'hand': 620, 'hands': 621, 'happ': 622, 'har': 623, 'hard': 624, 'has': 625, 'have': 626, 'having': 627, 'he': 628, 'head': 629, 'hear': 630, 'heard': 631, 'heart': 632, 'hel': 633, 'her': 634, 'here': 635, 'herself': 636, 'high': 637, 'him': 638, 'himself': 639, 'his': 640, 'ho': 641, 'home': 642, 'hor': 643, 'hour': 644, 'house': 645, 'how': 646, 'however': 647, 'hu': 648, 'hum': 649, 'hundred': 650, 'i': 651, 'id': 652, 'if': 653, 'ill': 654, 'im': 655, 'imp': 656, 'in': 657, 'inc': 658, 'ind': 659, 'inde': 660, 'inf': 661, 'ins': 662, 'inst': 663, 'int': 664, 'inter': 665, 'into': 666, 'is': 667, 'it': 668, 'its': 669, 'j': 670, 'jo': 671, 'john': 672, 'just': 673, 'k': 674, 'ke': 675, 'keep': 676, 'kind': 677, 'king': 678, 'kn': 679, 'knew': 680, 'know': 681, 'l': 682, 'la': 683, 'land': 684, 'lar': 685, 'last': 686, 'laugh': 687, 'lay': 688, 'le': 689, 'left': 690, 'less': 691, 'let': 692, 'li': 693, 'life': 694, 'light': 695, 'like': 696, 'little': 697, 'lo': 698, 'long': 699, 'look': 700, 'looked': 701, 'looking': 702, 'love': 703, 'm': 704, 'ma': 705, 'made': 706, 'make': 707, 'man': 708, 'many': 709, 'mar': 710, 'mas': 711, 'matter': 712, 'may': 713, 'me': 714, 'mean': 715, 'men': 716, 'mer': 717, 'met': 718, 'might': 719, 'min': 720, 'mind': 721, 'mis': 722, 'miss': 723, 'missus': 724, 'mister': 725, 'mo': 726, 'mom': 727, 'moment': 728, 'mon': 729, 'more': 730, 'morn': 731, 'morning': 732, 'most': 733, 'mother': 734, 'mu': 735, 'much': 736, 'must': 737, 'my': 738, 'myself': 739, 'n': 740, 'name': 741, 'nat': 742, 'ne': 743, 'near': 744, 'need': 745, 'never': 746, 'new': 747, 'next': 748, 'night': 749, 'no': 750, 'nor': 751, 'not': 752, 'nothing': 753, 'now': 754, 'o': 755, 'ob': 756, 'occ': 757, 'of': 758, 'off': 759, 'oh': 760, 'old': 761, 'on': 762, 'once': 763, 'one': 764, 'only': 765, 'op': 766, 'open': 767, 'or': 768, 'other': 769, 'others': 770, 'our': 771, 'out': 772, 'over': 773, 'own': 774, 'p': 775, 'par': 776, 'part': 777, 'pass': 778, 'pat': 779, 'pe': 780, 'peop': 781, 'people': 782, 'per': 783, 'perhaps': 784, 'pers': 785, 'person': 786, 'ph': 787, 'pl': 788, 'pla': 789, 'place': 790, 'play': 791, 'ple': 792, 'pleas': 793, 'po': 794, 'point': 795, 'poor': 796, 'poss': 797, 'power': 798, 'pr': 799, 'pre': 800, 'pres': 801, 'present': 802, 'prin': 803, 'pro': 804, 'prop': 805, 'pur': 806, 'put': 807, 'q': 808, 'qu': 809, 'quest': 810, 'quite': 811, 'r': 812, 'rather': 813, 're': 814, 'reach': 815, 'read': 816, 'red': 817, 'ref': 818, 'reg': 819, 'rel': 820, 'rem': 821, 'rep': 822, 'replied': 823, 'res': 824, 'rest': 825, 'ret': 826, 'return': 827, 'right': 828, 'ro': 829, 'room': 830, 'round': 831, 's': 832, 'sa': 833, 'said': 834, 'same': 835, 'sat': 836, 'saw': 837, 'say': 838, 'sc': 839, 'se': 840, 'see': 841, 'seem': 842, 'seemed': 843, 'seen': 844, 'sent': 845, 'ser': 846, 'serv': 847, 'set': 848, 'sh': 849, 'shall': 850, 'she': 851, 'should': 852, 'side': 853, 'sil': 854, 'sim': 855, 'sin': 856, 'sir': 857, 'six': 858, 'sk': 859, 'sl': 860, 'sle': 861, 'sm': 862, 'small': 863, 'sn': 864, 'so': 865, 'some': 866, 'somet': 867, 'something': 868, 'soon': 869, 'sou': 870, 'sp': 871, 'spe': 872, 'st': 873, 'stand': 874, 'ste': 875, 'still': 876, 'stood': 877, 'str': 878, 'stre': 879, 'su': 880, 'sub': 881, 'such': 882, 'sudden': 883, 'suff': 884, 'sun': 885, 'supp': 886, 'sur': 887, 'sure': 888, 'sw': 889, 'sy': 890, 't': 891, 'take': 892, 'te': 893, 'tell': 894, 'ter': 895, 'th': 896, 'than': 897, 'that': 898, 'the': 899, 'their': 900, 'them': 901, 'then': 902, 'there': 903, 'these': 904, 'they': 905, 'thing': 906, 'things': 907, 'think': 908, 'thir': 909, 'this': 910, 'those': 911, 'though': 912, 'thought': 913, 'thr': 914, 'three': 915, 'through': 916, 'tim': 917, 'time': 918, 'to': 919, 'together': 920, 'told': 921, 'too': 922, 'took': 923, 'toward': 924, 'tr': 925, 'tra': 926, 'tre': 927, 'turn': 928, 'turned': 929, 'tw': 930, 'two': 931, 'u': 932, 'un': 933, 'unc': 934, 'under': 935, 'underst': 936, 'unt': 937, 'until': 938, 'up': 939, 'upon': 940, 'us': 941, 'v': 942, 'very': 943, 'vis': 944, 'vo': 945, 'voice': 946, 'w': 947, 'wa': 948, 'want': 949, 'war': 950, 'was': 951, 'water': 952, 'way': 953, 'we': 954, 'well': 955, 'went': 956, 'were': 957, 'wh': 958, 'what': 959, 'when': 960, 'where': 961, 'which': 962, 'while': 963, 'white': 964, 'who': 965, 'whole': 966, 'whom': 967, 'why': 968, 'will': 969, 'wind': 970, 'wish': 971, 'with': 972, 'without': 973, 'wo': 974, 'wom': 975, 'woman': 976, 'wonder': 977, 'wood': 978, 'wor': 979, 'word': 980, 'words': 981, 'work': 982, 'world': 983, 'would': 984, 'wr': 985, 'x': 986, 'y': 987, 'year': 988, 'years': 989, 'yes': 990, 'yet': 991, 'you': 992, 'young': 993, 'your': 994, 'z': 995}\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(vocab)\n",
    "num_embeddings = len(vocab)\n",
    "print(num_embeddings)\n",
    "vocab_ids = [int(i) for i in range(num_embeddings)]\n",
    "vocab_dict = dict(zip(vocab,vocab_ids))\n",
    "print(f\"Character mappings : {vocab_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30db4bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e64652c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_tokenize(text):\n",
    "    text_list = re.split(\" \", text)\n",
    "    new_list = []\n",
    "    for i in range(0,len(text_list)-1):\n",
    "        new_list.append(text_list[i])\n",
    "        new_list.append(' ')\n",
    "    new_list.append(text_list[-1])   \n",
    "    #print(new_list)\n",
    "    encoded_words = []\n",
    "    for word in new_list:\n",
    "        encoded_words += encode_word(word)\n",
    "    print(encoded_words)\n",
    "    mapped_words = [vocab_dict[word_piece] for word_piece in encoded_words]\n",
    "    return mapped_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6176af31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ch', '##ap', '##ter', ' ', 'one', ' ', 'missus', ' ', 'r', '##ach', '##el', ' ', 'l', '##y', '##nd', '##e', ' ', 'is', ' ', 'sur', '##p', '##ri', '##se', '##d', ' ', 'missus', ' ', 'r', '##ach', '##el', ' ', 'l', '##y', '##nd', '##e', ' ', 'li', '##ved', ' ', 'just', ' ', 'where', ' ', 'the', ' ', 'a', '##v', '##on', '##le', '##a', ' ', 'ma', '##in', ' ', 'ro', '##ad', ' ', 'd', '##ip', '##pe', '##d', ' ', 'down', ' ', 'into', ' ', 'a', ' ', 'little', ' ', 'ho', '##llow', ' ', 'fr', '##ing', '##ed', ' ', 'with', ' ', 'al', '##der', '##s', ' ', 'and', ' ', 'la', '##d', '##ies', ' ', 'ear', '##d', '##ro', '##ps', ' ', 'and', ' ', 'tra', '##ver', '##se', '##d', ' ', 'by', ' ', 'a', ' ', 'br', '##ook']\n",
      "[500, 40, 360, 0, 764, 0, 724, 0, 812, 6, 105, 0, 682, 413, 252, 100, 0, 667, 0, 887, 318, 341, 348, 96, 0, 724, 0, 812, 6, 105, 0, 682, 413, 252, 100, 0, 693, 399, 0, 673, 0, 961, 0, 899, 0, 417, 397, 271, 236, 1, 0, 705, 187, 0, 829, 9, 0, 525, 200, 319, 96, 0, 542, 0, 666, 0, 417, 0, 697, 0, 641, 243, 0, 593, 192, 103, 0, 972, 0, 433, 99, 347, 0, 442, 0, 683, 96, 172, 0, 548, 96, 344, 328, 0, 442, 0, 926, 403, 348, 96, 0, 490, 0, 417, 0, 485, 280]\n"
     ]
    }
   ],
   "source": [
    "print(new_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "143b6e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ch##ap##ter one missus r##ach##el l##y##nd##e is sur##p##ri##se##d missus r##ach##el l##y##nd##e li##ved just where the a##v##on##le##a ma##in ro##ad d##ip##pe##d down into a little ho##llow fr##ing##ed with al##der##s and la##d##ies ear##d##ro##ps and tra##ver##se##d by a br##ook\n"
     ]
    }
   ],
   "source": [
    "sample = ['ch', '##ap', '##ter', ' ', 'one', ' ', 'missus', ' ', 'r', '##ach', '##el', ' ', 'l', '##y', '##nd', '##e', ' ', 'is', ' ', 'sur', '##p', '##ri', '##se', '##d', ' ', 'missus', ' ', 'r', '##ach', '##el', ' ', 'l', '##y', '##nd', '##e', ' ', 'li', '##ved', ' ', 'just', ' ', 'where', ' ', 'the', ' ', 'a', '##v', '##on', '##le', '##a', ' ', 'ma', '##in', ' ', 'ro', '##ad', ' ', 'd', '##ip', '##pe', '##d', ' ', 'down', ' ', 'into', ' ', 'a', ' ', 'little', ' ', 'ho', '##llow', ' ', 'fr', '##ing', '##ed', ' ', 'with', ' ', 'al', '##der', '##s', ' ', 'and', ' ', 'la', '##d', '##ies', ' ', 'ear', '##d', '##ro', '##ps', ' ', 'and', ' ', 'tra', '##ver', '##se', '##d', ' ', 'by', ' ', 'a', ' ', 'br', '##ook']\n",
    "sample_text = ''\n",
    "for i in range(len(sample)):\n",
    "    sample_text += sample[i]\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92f6d1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chapter one missus rachel lynde is surprised missus rachel lynde lived just where the avonlea main road dipped down into a little hollow fringed with alders and ladies eardrops and traversed by a brook\n"
     ]
    }
   ],
   "source": [
    "my_new_string = sample_text.replace(\"#\", \"\")\n",
    "print(my_new_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
